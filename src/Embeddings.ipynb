{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/SMSSpamCollection', sep='\\t', header=None)\n",
    "data['Text'] = data[1].str.replace('[^\\w\\s]','')\n",
    "data.columns = ['label', 'Full Text', 'Text']\n",
    "data['Lower Case Text'] = data['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "labels, counts = np.unique(data['label'], return_counts=True)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(labels[np.argsort(-counts)])\n",
    "data['y'] = encoder.transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mask_train = np.random.random(data.shape[0]) < 0.8\n",
    "data_train = data[mask_train]\n",
    "data_test = data.iloc[~mask_train, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#up sample data train for word2vec vocabulary\n",
    "countToIncrease_word = data_train[data_train['y'] == 0].shape[0] - data_train[data_train['y'] == 1].shape[0]\n",
    "spamupsampled_word = data_train[data_train['y'] == 1].sample(n=countToIncrease_word, replace=True)\n",
    "data_train_upsample_word2vec = pd.concat([spamupsampled_word, data_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_sing_word = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "count_vect_sing_word.fit(data_train_upsample_word2vec['Lower Case Text'])\n",
    "tokenizer_word = count_vect_sing_word.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(count_vect_sing_word.vocabulary_)\n",
    "EMBEDDING_SIZE = 300\n",
    "word_to_ix = count_vect_sing_word.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(TwoGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 1\n",
    "EMBEDDING_DIM = EMBEDDING_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = TwoGramLanguageModeler(VOCAB_SIZE, EMBEDDING_DIM, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoGramLanguageModeler(\n",
       "  (embeddings): Embedding(8111, 300)\n",
       "  (linear1): Linear(in_features=300, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=8111, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = '../data/word_2vec_model'\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "word2vec_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "word2vec_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 9279)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Word Embeddings\n",
    "\n",
    "a = ENGLISH_STOP_WORDS\n",
    "\n",
    "full_data = data['Text'].tolist()\n",
    "\n",
    "cv = CountVectorizer(binary = True, stop_words = a)\n",
    "\n",
    "full_data_transformed = cv.fit_transform(full_data)\n",
    "fd = pd.DataFrame(full_data_transformed.toarray(), columns = cv.get_feature_names())\n",
    "fd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>1085</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>1863</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>2370</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5104</th>\n",
       "      <td>5104</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>2434</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index   0\n",
       "1085   1085  53\n",
       "1863   1863  51\n",
       "2370   2370  51\n",
       "5104   5104  51\n",
       "2434   2434  49"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums = pd.DataFrame(fd.sum(axis=1))\n",
    "sums = sums.reset_index()\n",
    "sums.sort_values(by=0, ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect_sing_word is a CountVectorizer\n",
    "def _indicesForSentence(input_str, tokenizer = tokenizer_word, count_vect = count_vect_sing_word):\n",
    "    input_str = list(filter(lambda x: x in count_vect.vocabulary_, tokenizer(input_str)))\n",
    "    return torch.tensor([[word_to_ix[word]] for word in input_str], dtype=torch.long)\n",
    "\n",
    "def sentenceToNumpyInstance(input_str, embedder):\n",
    "    embeddings = embedder(_indicesForSentence(input_str))\n",
    "    if embeddings.shape == torch.Size([0]):\n",
    "        return np.zeros(EMBEDDING_SIZE)\n",
    "    else:\n",
    "        return torch.Tensor.numpy(embeddings.detach())\n",
    "    \n",
    "def word2vec_transform(data, embeddings, field = 'Lower Case Text'):\n",
    "    return np.array(data[field].apply(sentenceToNumpyInstance, embedder=embeddings).values.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for item in count_vect_sing_word.vocabulary_.items():\n",
    "    words.append(item[0])\n",
    "words = np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_vocab = []\n",
    "for w in words:\n",
    "    embeddings_vocab.append(sentenceToNumpyInstance(w, embedder = word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_vocab_tmp = []\n",
    "for e in embeddings_vocab:\n",
    "    embeddings_vocab_tmp.append(e[0][0])\n",
    "embeddings_vocab = np.array(embeddings_vocab_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8111, 300)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = embeddings_vocab\n",
    "vec1 = embs[1085]\n",
    "vec2 = embs[1863]\n",
    "vec3 = embs[2370]\n",
    "vec4 = embs[5104]\n",
    "vec5 = embs[2434]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.18981096,  1.4227369 , -0.9902384 ,  1.2292829 ,  0.98955137,\n",
       "        0.923227  , -0.18684249,  0.3918733 ,  0.7780009 , -0.9048305 ,\n",
       "       -2.1496806 , -0.13471963, -0.36963987,  0.582406  ,  0.18956366,\n",
       "        0.4845125 , -0.47391316, -0.43434104, -0.93856424,  0.11929535,\n",
       "       -1.4997768 , -1.2167685 ,  0.20231733, -0.36255556, -0.91724104,\n",
       "        0.39457127, -0.860566  , -0.4210809 ,  2.0554852 , -0.16308868,\n",
       "       -0.32135576,  0.3605507 , -0.26749006, -0.74755085, -0.47850364,\n",
       "        0.2970824 ,  0.1993891 ,  0.29878286,  0.3082507 , -1.2246063 ,\n",
       "       -1.6436683 ,  1.7239445 ,  2.2355354 ,  0.31730568, -0.4736394 ,\n",
       "       -1.2710063 ,  1.2342807 ,  0.06797076,  0.41186175, -1.1168177 ,\n",
       "       -0.16750985,  1.7197348 , -1.4376853 , -1.8525515 ,  0.12735182,\n",
       "        0.9485533 , -1.480798  ,  0.03132405,  0.6196161 ,  0.22978495,\n",
       "       -1.7416118 ,  1.5940908 , -1.8554853 ,  0.20038296,  1.9638414 ,\n",
       "        0.61478215, -1.1431602 ,  0.8820961 ,  0.6057215 , -0.58140075,\n",
       "       -1.0782956 ,  0.8643247 , -0.53696615, -1.6015813 , -0.0687862 ,\n",
       "       -0.00503359, -0.90229464,  0.5585406 , -1.028404  ,  0.36746058,\n",
       "        1.3533646 , -0.4867612 , -0.33097062,  0.09385575,  0.5519473 ,\n",
       "       -0.58951974,  0.512902  , -0.46972573, -0.39848065,  0.19196573,\n",
       "        0.9698547 , -1.5171438 , -2.1984024 , -0.13575159,  0.75848037,\n",
       "        0.20324688, -1.033663  , -0.250649  ,  0.8993153 ,  0.40421268,\n",
       "        0.31335655,  0.65896803,  0.11113218, -0.75403905,  0.9692653 ,\n",
       "       -0.5879939 , -0.21587303,  0.33482292,  0.23996761, -0.97448856,\n",
       "        1.6413659 ,  0.72108144, -0.41639566, -2.4138877 ,  0.9941603 ,\n",
       "        0.34016144, -0.18661946,  1.02119   ,  0.25529313, -0.30985126,\n",
       "       -0.31237227,  0.39874753, -1.0360087 ,  1.1862005 , -0.20100102,\n",
       "       -0.78022367,  0.19355589,  1.0860761 ,  0.52877283, -1.1555469 ,\n",
       "        0.08384918, -0.20425647, -1.1547484 ,  0.84317255, -0.3532697 ,\n",
       "       -0.9010414 ,  0.93034   , -1.2994168 ,  0.2167956 ,  1.4357994 ,\n",
       "       -0.47990513, -0.72718185,  0.14566025,  0.14203767, -0.0928677 ,\n",
       "       -0.4909287 ,  1.4888986 , -0.8309433 ,  0.07783898, -1.2467523 ,\n",
       "       -0.24673826, -0.25758874, -1.4718539 , -1.481709  , -0.58506775,\n",
       "       -1.5156188 ,  0.1069392 , -1.5258998 , -1.8828471 ,  1.3742294 ,\n",
       "        0.02367726,  1.0270398 , -1.0221573 , -0.55090606,  0.30213374,\n",
       "        1.0518348 , -1.4368643 ,  0.5403563 ,  0.56745094, -0.04573627,\n",
       "        1.5540333 ,  0.9509238 ,  1.0673857 ,  0.33023918, -0.24930413,\n",
       "       -1.2101794 , -0.17536797,  0.07587628, -0.06777606,  1.6854899 ,\n",
       "       -0.2092133 ,  0.6043203 , -1.446221  ,  0.46200466, -0.5520253 ,\n",
       "       -1.6494784 , -1.2331384 ,  0.0220417 ,  0.84798706,  1.0984205 ,\n",
       "       -0.10305175, -0.68907094,  1.4443107 , -0.15750062,  1.481702  ,\n",
       "       -0.01594302, -0.01769083,  0.7254778 , -0.8984738 ,  1.3001125 ,\n",
       "        0.0986437 , -0.49853316,  2.2490997 ,  0.6954333 , -0.3476175 ,\n",
       "       -0.3174585 , -0.55472636,  0.77589583, -1.732044  ,  0.16680662,\n",
       "       -0.68445575, -1.0038291 ,  0.61102176,  0.64846605,  0.22610772,\n",
       "        0.3279334 ,  0.66295636,  1.0608565 ,  1.6147523 ,  0.559702  ,\n",
       "        1.177765  ,  0.19109489, -0.1373001 , -0.25401992, -1.1736245 ,\n",
       "       -0.07632596,  0.14354399,  1.885348  , -0.31363815,  0.41579717,\n",
       "       -0.37425324, -1.1847047 ,  0.98603666, -0.46752855,  0.97599655,\n",
       "       -0.46955755, -1.1867709 , -0.740977  ,  1.945036  , -0.07056455,\n",
       "       -0.45752487,  0.33068728,  0.14189033,  1.3501722 , -0.60895234,\n",
       "        0.9171455 , -0.76297224,  0.54326415,  0.60914993, -0.44076893,\n",
       "        0.37331808, -0.80465305,  0.11246783, -1.7017719 ,  1.2779118 ,\n",
       "       -1.2794026 , -1.1276634 ,  0.21983758,  0.9181411 , -0.4130017 ,\n",
       "        0.25987056, -1.2622437 , -1.3400407 ,  1.6297277 ,  1.1081805 ,\n",
       "       -0.40937054,  0.6878965 , -0.9259038 , -0.48208198,  0.7018216 ,\n",
       "        0.43780622,  1.5926977 , -0.05169945,  0.82149637, -1.458443  ,\n",
       "       -0.27207178,  1.6105766 ,  0.00295959,  0.47806045,  1.0462475 ,\n",
       "        0.7570869 ,  0.3117886 ,  0.52913445,  0.6397596 , -0.20790987,\n",
       "        0.7707879 , -0.35341877,  0.03534248, -0.18619223,  0.3752083 ,\n",
       "       -0.16767822, -1.0218018 ,  1.0781142 , -0.9678773 , -0.00432666,\n",
       "        0.06480277, -0.2759423 ,  1.8766818 , -2.460006  ,  0.09108936],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Max word embedding \n",
    "print(vec1.shape)\n",
    "vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-febaf140e5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "%matplotlib inline\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "\n",
    "transformed_embs = reducer.fit_transform(embs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-8176d1f0922a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(transformed_embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
