{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little bit of data preprocessing. Rest happens inside cross validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/SMSSpamCollection', sep='\\t', header=None)\n",
    "data['Text'] = data[1].str.replace('[^\\w\\s]','')\n",
    "data.columns = ['label', 'Full Text', 'Text']\n",
    "data['Lower Case Text'] = data['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(data['label'], return_counts=True)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(labels[np.argsort(-counts)])\n",
    "data['y'] = encoder.transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mask_train = np.random.random(data.shape[0]) < 0.8\n",
    "data_train = data[mask_train]\n",
    "data_test = data.iloc[~mask_train, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word2vec stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#up sample data train for word2vec vocabulary\n",
    "countToIncrease_word = data_train[data_train['y'] == 0].shape[0] - data_train[data_train['y'] == 1].shape[0]\n",
    "spamupsampled_word = data_train[data_train['y'] == 1].sample(n=countToIncrease_word, replace=True)\n",
    "data_train_upsample_word2vec = pd.concat([spamupsampled_word, data_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_sing_word = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "count_vect_sing_word.fit(data_train_upsample_word2vec['Lower Case Text'])\n",
    "tokenizer_word = count_vect_sing_word.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(count_vect_sing_word.vocabulary_)\n",
    "EMBEDDING_SIZE = 300\n",
    "word_to_ix = count_vect_sing_word.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(TwoGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 1\n",
    "EMBEDDING_DIM = EMBEDDING_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = TwoGramLanguageModeler(VOCAB_SIZE, EMBEDDING_DIM, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoGramLanguageModeler(\n",
       "  (embeddings): Embedding(8111, 300)\n",
       "  (linear1): Linear(in_features=300, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=8111, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = '../data/word_2vec_model'\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "word2vec_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "word2vec_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect_sing_word is a CountVectorizer\n",
    "def _indicesForSentence(input_str, tokenizer = tokenizer_word, count_vect = count_vect_sing_word):\n",
    "    input_str = list(filter(lambda x: x in count_vect.vocabulary_, tokenizer(input_str)))\n",
    "    return torch.tensor([[word_to_ix[word]] for word in input_str], dtype=torch.long)\n",
    "\n",
    "def getEmbedding(word_index_tensor, embedder):\n",
    "    embedder(word_index_tensor)\n",
    "def sentenceToNumpyInstance(input_str, embedder):\n",
    "    embeddings = embedder(_indicesForSentence(input_str))\n",
    "    if embeddings.shape == torch.Size([0]):\n",
    "        return np.zeros(EMBEDDING_SIZE)\n",
    "    else:\n",
    "        x = torch.sum(embeddings, dim = 0)\n",
    "        return torch.Tensor.numpy(x.detach())[0]\n",
    "    \n",
    "def word2vec_transform(data, embeddings, field = 'Lower Case Text'):\n",
    "    return np.array(data[field].apply(sentenceToNumpyInstance, embedder=embeddings).values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ending of Word2Vec initialization\n",
    "\n",
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is supposed to be the normal trained data\n",
    "## vectorizer is supposed to be the constructor\n",
    "def crossval(dataset, model_func, parametersModel, embeddings = word_embeddings, vectorizer = None, \n",
    "             label_name = 'y', text_field = 'Lower Case Text', k_folds = 5): \n",
    "    aucs = {}\n",
    "    folds = list(KFold(n_splits=k_folds).split(dataset))\n",
    "    for fold in range(k_folds):\n",
    "        cv_train = dataset.iloc[folds[fold][0]]\n",
    "        cv_validate = dataset.iloc[folds[fold][1]]\n",
    "    \n",
    "        countToIncrease = cv_train[cv_train['y'] == 0].shape[0] - cv_train[cv_train['y'] == 1].shape[0]\n",
    "        spamupsampled = cv_train[cv_train['y'] == 1].sample(n=countToIncrease, replace=True)\n",
    "        cv_train = pd.concat([spamupsampled, cv_train])\n",
    "        \n",
    "        if vectorizer is not None:\n",
    "            cv_train_features = vectorizer.fit_transform(cv_train[text_field])\n",
    "            cv_train_y = cv_train[label_name]\n",
    "            cv_validate_features = vectorizer.transform(cv_validate[text_field])\n",
    "            cv_validate_y = cv_validate[label_name]\n",
    "        else:\n",
    "            # LOAD word to vec\n",
    "            cv_train_features = word2vec_transform(cv_train, embeddings)\n",
    "            cv_train_y = cv_train[label_name]\n",
    "            cv_validate_features = word2vec_transform(cv_validate, embeddings)\n",
    "            cv_validate_y = cv_validate[label_name]\n",
    "        \n",
    "        keys, values = zip(*parametersModel.items())\n",
    "                \n",
    "        for v in itertools.product(*values):    \n",
    "            experiment = dict(zip(keys, v))\n",
    "            m = model_func(**experiment)\n",
    "\n",
    "            m.fit(cv_train_features, cv_train_y)\n",
    "            y_preds = m.predict_proba(cv_validate_features)[:, 1]\n",
    "            fpr, tpr, threshold = metrics.roc_curve(cv_validate_y, y_preds)\n",
    "            auc_val = metrics.auc(fpr, tpr)  \n",
    "            \n",
    "            inside_key = \"\"\n",
    "\n",
    "            for e in experiment:\n",
    "                inside_key = inside_key + \"_\" + e + ':' + str(experiment[e])\n",
    "            \n",
    "            if not inside_key in aucs:\n",
    "                aucs[inside_key] = []\n",
    "                aucs[inside_key].append(auc_val)\n",
    "            else:\n",
    "                aucs[inside_key].append(auc_val)\n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_parameters = {'penalty': ['l1', 'l2'], 'C': [1, 5, 10]}\n",
    "\n",
    "count_vect = CountVectorizer(min_df=2, stop_words=ENGLISH_STOP_WORDS)\n",
    "tfidf_vect = TfidfVectorizer(min_df=2, stop_words=ENGLISH_STOP_WORDS)\n",
    "bigram_count_vect = CountVectorizer(min_df=2, stop_words=ENGLISH_STOP_WORDS, ngram_range = (2,2))\n",
    "bigram_tfidf_vect = TfidfVectorizer(min_df=2, stop_words=ENGLISH_STOP_WORDS, ngram_range = (2,2))\n",
    "ngram_count_vect = CountVectorizer(min_df=2, stop_words=ENGLISH_STOP_WORDS, ngram_range = (1,2))\n",
    "ngram_tfidf_vect = TfidfVectorizer(min_df=2, stop_words=ENGLISH_STOP_WORDS, ngram_range = (1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_count_default = crossval(data_train, LogisticRegression, LR_parameters, vectorizer = count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_tfidf_default = crossval(data_train, LogisticRegression, LR_parameters, vectorizer = tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_count_bigram = crossval(data_train, LogisticRegression, LR_parameters, vectorizer = bigram_count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_tfidf_bigram = crossval(data_train, LogisticRegression, LR_parameters, vectorizer = bigram_tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_count_ngram = crossval(data_train, LogisticRegression, LR_parameters, vectorizer = ngram_count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_tfidf_ngram = crossval(data_train, LogisticRegression, LR_parameters, vectorizer = ngram_tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults to word 2 vec\n",
    "aucs_word_2_vec = crossval(data_train, LogisticRegression, LR_parameters, vectorizer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAUCsResults(aucs_dict):\n",
    "    aucs = {}\n",
    "    max_auc = -1\n",
    "    confs = \"\"\n",
    "    for e in aucs_dict:\n",
    "        mean_auc = np.mean(aucs_dict[e])\n",
    "        aucs['LR'+ e] = mean_auc\n",
    "        if mean_auc >= max_auc:\n",
    "            confs = e\n",
    "            max_auc = mean_auc\n",
    "    return aucs, confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'LR_penalty:l1_C:1': 0.9785991319005006, 'LR_penalty:l1_C:5': 0.9808460754822054, 'LR_penalty:l1_C:10': 0.9819221616286926, 'LR_penalty:l2_C:1': 0.9889649363700389, 'LR_penalty:l2_C:5': 0.989242491721642, 'LR_penalty:l2_C:10': 0.9892710516402273}, '_penalty:l2_C:10')\n"
     ]
    }
   ],
   "source": [
    "print(processAUCsResults(aucs_count_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'LR_penalty:l1_C:1': 0.9768598339157105, 'LR_penalty:l1_C:5': 0.9791266912843237, 'LR_penalty:l1_C:10': 0.9795823131957532, 'LR_penalty:l2_C:1': 0.9886230964977036, 'LR_penalty:l2_C:5': 0.9896713875234973, 'LR_penalty:l2_C:10': 0.9898372840630074}, '_penalty:l2_C:10')\n"
     ]
    }
   ],
   "source": [
    "print(processAUCsResults(aucs_tfidf_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'LR_penalty:l1_C:1': 0.8805183412669118, 'LR_penalty:l1_C:5': 0.8888743386586515, 'LR_penalty:l1_C:10': 0.892444811244621, 'LR_penalty:l2_C:1': 0.9454633349453758, 'LR_penalty:l2_C:5': 0.9454256474566112, 'LR_penalty:l2_C:10': 0.9456754610212901}, '_penalty:l2_C:10')\n"
     ]
    }
   ],
   "source": [
    "print(processAUCsResults(aucs_count_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'LR_penalty:l1_C:1': 0.8791368988367461, 'LR_penalty:l1_C:5': 0.8891661832287282, 'LR_penalty:l1_C:10': 0.8910393322909524, 'LR_penalty:l2_C:1': 0.9445047820735075, 'LR_penalty:l2_C:5': 0.9442792840158292, 'LR_penalty:l2_C:10': 0.945394088013553}, '_penalty:l2_C:10')\n"
     ]
    }
   ],
   "source": [
    "print(processAUCsResults(aucs_tfidf_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'LR_penalty:l1_C:1': 0.9778788794787188, 'LR_penalty:l1_C:5': 0.9806106828778901, 'LR_penalty:l1_C:10': 0.9825216892117155, 'LR_penalty:l2_C:1': 0.989369208823606, 'LR_penalty:l2_C:5': 0.9899661401496254, 'LR_penalty:l2_C:10': 0.9901976985866373}, '_penalty:l2_C:10')\n"
     ]
    }
   ],
   "source": [
    "print(processAUCsResults(aucs_count_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'LR_penalty:l1_C:1': 0.9740342938614649, 'LR_penalty:l1_C:5': 0.977640473783844, 'LR_penalty:l1_C:10': 0.9778844338458083, 'LR_penalty:l2_C:1': 0.9858649183438238, 'LR_penalty:l2_C:5': 0.98763164865167, 'LR_penalty:l2_C:10': 0.9879518962811862}, '_penalty:l2_C:10')\n"
     ]
    }
   ],
   "source": [
    "print(processAUCsResults(aucs_tfidf_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'LR_penalty:l1_C:1': 0.9261628207083324, 'LR_penalty:l1_C:5': 0.9223089021148987, 'LR_penalty:l1_C:10': 0.9212310045362887, 'LR_penalty:l2_C:1': 0.9261725350181171, 'LR_penalty:l2_C:5': 0.9238190049716044, 'LR_penalty:l2_C:10': 0.9228287948871827}, '_penalty:l2_C:1')\n"
     ]
    }
   ],
   "source": [
    "print(processAUCsResults(aucs_word_2_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run through with all training data on best parameters:\n",
    "\n",
    "# need to upsample the data_train\n",
    "# fit the vectorizer\n",
    "# model should be an instantiated model\n",
    "# returns the TEST AUC on normal rate and upsampled test\n",
    "def train_test_run(data_train, data_test, model, embeddings = word_embeddings, vectorizer = None, \n",
    "                   label_name = 'y', text_field = 'Lower Case Text'):\n",
    "\n",
    "    countToIncrease = data_train[data_train['y'] == 0].shape[0] - data_train[data_train['y'] == 1].shape[0]\n",
    "    spamupsampled = data_train[data_train['y'] == 1].sample(n=countToIncrease, replace=True)\n",
    "    data_train = pd.concat([spamupsampled, data_train])\n",
    "    \n",
    "    downsampled_ham = data_test[data_test['y'] == 0].sample(n=data_test[data_test['y'] == 1].shape[0])\n",
    "    # the name really should be _downsample, but i dont wanna fix code right now\n",
    "    data_test_upsample = pd.concat([downsampled_ham, data_test[data_test['y'] == 1]])\n",
    "\n",
    "    if vectorizer is not None:\n",
    "        data_train_features = vectorizer.fit_transform(data_train[text_field])\n",
    "        data_train_y = data_train[label_name]\n",
    "        data_test_features = vectorizer.transform(data_test[text_field])\n",
    "        data_test_y = data_test[label_name]\n",
    "        data_test_upsample_features = vectorizer.transform(data_test_upsample[text_field])\n",
    "        data_test_upsample_y = data_test_upsample[label_name]\n",
    "    else:\n",
    "        # LOAD word to vec\n",
    "        data_train_features = word2vec_transform(data_train, embeddings)\n",
    "        data_train_y = data_train[label_name]\n",
    "        data_test_features = word2vec_transform(data_test, embeddings)\n",
    "        data_test_y = data_test[label_name]\n",
    "        data_test_upsample_features = word2vec_transform(data_test_upsample, embeddings)\n",
    "        data_test_upsample_y = data_test_upsample[label_name]\n",
    "        \n",
    "    model.fit(data_train_features, data_train_y)\n",
    "    y_preds = model.predict_proba(data_test_features)[:, 1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(data_test_y, y_preds)\n",
    "    y_preds_upsample = model.predict_proba(data_test_upsample_features)[:, 1]\n",
    "    fpr_upsample, tpr_upsample, threshold_upsample = metrics.roc_curve(data_test_upsample_y, y_preds_upsample)\n",
    "    return metrics.auc(fpr, tpr), metrics.auc(fpr_upsample, tpr_upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9837854603125878, 0.9847946000870953)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_run(data_train, data_test, LogisticRegression(penalty='l2', C = 10), vectorizer=count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9905641674980176, 0.9915263463492524)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_run(data_train, data_test, LogisticRegression(penalty='l2', C = 10), vectorizer=tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9389244877599571, 0.9391239657424881)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_run(data_train, data_test, LogisticRegression(penalty='l2', C = 10), vectorizer=bigram_count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9413481953290871, 0.9391058208738567)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_run(data_train, data_test, LogisticRegression(penalty='l2', C = 10), vectorizer=bigram_tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9849045864988616, 0.984994193642038)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_run(data_train, data_test, LogisticRegression(penalty='l2', C = 10), vectorizer=ngram_count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9906792776200344, 0.9927964871534329)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_run(data_train, data_test, LogisticRegression(penalty='l2', C = 10), vectorizer=ngram_tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9267899623973601, 0.9250072579474525)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defaults to word 2 vec\n",
    "train_test_run(data_train, data_test, LogisticRegression(penalty='l2', C = 1), vectorizer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
