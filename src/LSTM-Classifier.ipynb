{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 0.001\n",
    "HIDDEN_DIM = 5\n",
    "GRADIENT_CLIP = 6\n",
    "DROPOUT_PARAM = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(TwoGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/SMSSpamCollection', sep='\\t', header=None)\n",
    "data['Text'] = data[1].str.replace('[^\\w\\s]','')\n",
    "data.columns = ['label', 'Full Text', 'Text']\n",
    "data['Lower Case Text'] = data['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(data['label'], return_counts=True)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(labels[np.argsort(-counts)])\n",
    "data['y'] = encoder.transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mask_train = np.random.random(data.shape[0]) < 0.8\n",
    "data_train = data[mask_train]\n",
    "data_test = data.iloc[~mask_train, :]\n",
    "\n",
    "\n",
    "#up sample data train for word2vec vocabulary\n",
    "countToIncrease_word = data_train[data_train['y'] == 0].shape[0] - data_train[data_train['y'] == 1].shape[0]\n",
    "if (countToIncrease_word +  data_train[data_train['y'] == 1].shape[0]) % BATCH_SIZE != 0:\n",
    "    countToIncrease_word = countToIncrease_word + 1\n",
    "spamupsampled_word = data_train[data_train['y'] == 1].sample(n=countToIncrease_word, replace=True)\n",
    "data_train_upsample_word2vec = pd.concat([spamupsampled_word, data_train[data_train['y'] == 1]])\\\n",
    "                               .sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "count_vect_sing_word = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "count_vect_sing_word.fit(data_train_upsample_word2vec['Lower Case Text'])\n",
    "tokenizer_word = count_vect_sing_word.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = len(data_test) % BATCH_SIZE\n",
    "data_test_spam = data_test[data_test['y'] == 1]\n",
    "\n",
    "data_test_downsample_ham = data_test[data_test['y'] == 0].sample(len(data_test[data_test['y'] == 0]) - diff)\n",
    "data_test_downsample = pd.concat([data_test_spam, data_test_downsample_ham]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data down sample balanced\n",
    "## won't work for general batch_sizes\n",
    "test_spam_count = data_test[data_test['y'] == 1].shape[0]\n",
    "diff = test_spam_count % BATCH_SIZE \n",
    "test_spam_count = test_spam_count - diff\n",
    "data_test_downsample_spam = data_test[data_test['y'] == 1].sample(test_spam_count)\n",
    "\n",
    "data_test_downsample_ham = data_test[data_test['y'] == 0].sample(test_spam_count)\n",
    "data_test_downsample_bal = pd.concat([data_test_downsample_ham, data_test_downsample_spam]) \\\n",
    "    .sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "CONTEXT_SIZE = 1\n",
    "VOCAB_SIZE = len(count_vect_sing_word.vocabulary_)\n",
    "word_to_ix = count_vect_sing_word.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = TwoGramLanguageModeler(VOCAB_SIZE, EMBEDDING_SIZE, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TwoGramLanguageModeler:\n\tsize mismatch for embeddings.weight: copying a param of torch.Size([2412, 300]) from checkpoint, where the shape is torch.Size([8111, 300]) in current model.\n\tsize mismatch for linear2.weight: copying a param of torch.Size([2412, 128]) from checkpoint, where the shape is torch.Size([8111, 128]) in current model.\n\tsize mismatch for linear2.bias: copying a param of torch.Size([2412]) from checkpoint, where the shape is torch.Size([8111]) in current model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-14f8578475fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMODEL_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/word_2vec_model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 719\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TwoGramLanguageModeler:\n\tsize mismatch for embeddings.weight: copying a param of torch.Size([2412, 300]) from checkpoint, where the shape is torch.Size([8111, 300]) in current model.\n\tsize mismatch for linear2.weight: copying a param of torch.Size([2412, 128]) from checkpoint, where the shape is torch.Size([8111, 128]) in current model.\n\tsize mismatch for linear2.bias: copying a param of torch.Size([2412]) from checkpoint, where the shape is torch.Size([8111]) in current model."
     ]
    }
   ],
   "source": [
    "MODEL_PATH = '../data/word_2vec_model'\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "word2vec_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "word2vec_model.eval()\n",
    "\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "# TO FIX EMBEDDINGS\n",
    "word_embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect_sing_word is a CountVectorizer\n",
    "def _indicesForSentence(input_str, tokenizer = tokenizer_word, count_vect = count_vect_sing_word):\n",
    "    input_str = list(filter(lambda x: x in count_vect.vocabulary_, tokenizer(input_str)))\n",
    "    return torch.tensor([[word_to_ix[word]] for word in input_str], dtype=torch.long)\n",
    "\n",
    "def sentenceToNumpyInstance(input_str, embedder):\n",
    "    embeddings = embedder(_indicesForSentence(input_str))\n",
    "    if embeddings.shape == torch.Size([0]):\n",
    "        return np.zeros(EMBEDDING_SIZE)\n",
    "    else:\n",
    "        return torch.Tensor.numpy(embeddings.detach())\n",
    "    \n",
    "def word2vec_transform(data, embeddings, field = 'Lower Case Text'):\n",
    "    return np.array(data[field].apply(sentenceToNumpyInstance, embedder=embeddings).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data = word2vec_transform(data_train_upsample_word2vec, embeddings=word_embeddings)\n",
    "trans_test_data = word2vec_transform(data_test_downsample, embeddings=word_embeddings)\n",
    "trans_test_data_bal = word2vec_transform(data_test_downsample_bal, embeddings=word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentenceLengths(data):\n",
    "    sentence_lengths= []\n",
    "    for i in range(len(data)):\n",
    "        e = data[i]\n",
    "        if len(e.shape) > 1:\n",
    "            sentence_lengths.append(e.shape[0])\n",
    "        else:\n",
    "            sentence_lengths.append(1)\n",
    "    return sentence_lengths\n",
    "\n",
    "sentence_lengths = np.array(generateSentenceLengths(trans_data))\n",
    "indices_rv = np.argsort(-sentence_lengths)\n",
    "sentence_lengths = sentence_lengths[indices_rv]\n",
    "trans_data = trans_data[indices_rv]\n",
    "\n",
    "sentence_lengths_test = np.array(generateSentenceLengths(trans_test_data))\n",
    "indices_rv_test = np.argsort(-sentence_lengths_test)\n",
    "sentence_lengths_test = sentence_lengths_test[indices_rv_test]\n",
    "trans_test_data = trans_test_data[indices_rv_test]\n",
    "\n",
    "sentence_lengths_test_bal = np.array(generateSentenceLengths(trans_test_data_bal))\n",
    "indices_rv_test = np.argsort(-sentence_lengths_test_bal)\n",
    "sentence_lengths_test_bal = sentence_lengths_test_bal[indices_rv_test]\n",
    "trans_test_data_bal = trans_test_data_bal[indices_rv_test]\n",
    "\n",
    "\n",
    "assert sentence_lengths[0] >= sentence_lengths_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_so_far = -1\n",
    "for i in range(len(trans_data)):\n",
    "    e = trans_data[i]\n",
    "    if e.shape[0] >= max_len_so_far and len(e.shape) > 1:\n",
    "        max_len_so_far = e.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LEN = max_len_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = LEARNING_RATE * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim = EMBEDDING_SIZE, hidden_dim = HIDDEN_DIM, \\\n",
    "                 label_size = 2, batch_size = BATCH_SIZE, num_layers = 2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, dropout = DROPOUT_PARAM)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def forward(self, embeds, sentence_lengths):\n",
    "        print(embeds.shape)\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, sentence_lengths, batch_first=True)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        #lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        #y = self.hidden2label(lstm_out.float()[:, -1, :])\n",
    "        \n",
    "        # use h_t and last layer\n",
    "        y = self.hidden2label(self.hidden[0][-1].float())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSentences(data):\n",
    "    trans_data_reshape = np.zeros((data.shape[0], SENTENCE_LEN, EMBEDDING_SIZE))\n",
    "    # this will also do padding\n",
    "    for i in range(data.shape[0]):\n",
    "        e = data[i]\n",
    "        if len(e.shape) > 1:\n",
    "            sentence_len_sofar = e.shape[0]\n",
    "            for j in range(sentence_len_sofar):\n",
    "                trans_data_reshape[i, j] = e[j][0]\n",
    "    return trans_data_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data = padSentences(trans_data)\n",
    "trans_test_data = padSentences(trans_test_data)\n",
    "trans_test_data_bal = padSentences(trans_test_data_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trans_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data, sen_len):\n",
    "    return model(torch.tensor(data,dtype=torch.float), sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = data_train_upsample_word2vec['y']\n",
    "test_y = data_test_downsample['y']\n",
    "test_y_bal = data_test_downsample_bal['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "test_bal_loss_ = []\n",
    "train_acc_ = []\n",
    "test_acc_ = []\n",
    "test_bal_acc_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_LSTM = '../data/LSTM_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "train_predictions_tmp = []\n",
    "test_predictions_tmp = []\n",
    "test_bal_predictions_tmp = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print(epoch)\n",
    "    # using adam so might be too much\n",
    "    #optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_data.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_data[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(train_y.iloc[this_batch].values))\n",
    "        #inner_y.weight.requires_grad = False\n",
    "        batch_lengths = sentence_lengths[this_batch]\n",
    "        model.zero_grad()\n",
    "        \n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        train_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    train_loss_.append(1.0 * total_loss / total)\n",
    "    train_acc_.append(1.0 * total_acc.float() / total)\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_test_data.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_test_data[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(test_y.iloc[this_batch].values))\n",
    "        batch_lengths = sentence_lengths_test[this_batch]\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        test_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    test_loss_.append(total_loss / total)\n",
    "    test_acc_.append(total_acc.float() / total)\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_test_data_bal.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_test_data_bal[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(test_y_bal.iloc[this_batch].values))\n",
    "        batch_lengths = sentence_lengths_test_bal[this_batch]\n",
    "\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        test_bal_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    test_bal_loss_.append(total_loss / total)\n",
    "    test_bal_acc_.append(total_acc.float() / total)\n",
    "\n",
    "    print(('[Epoch: %3d/%3d] Training Loss: %.6f, Testing Loss: %.6f, Test Balanced Loss: %.6f, '+\n",
    "          'Training Acc: %.6f, Testing Acc: %.6f, Testing Balanced Acc: %.6f')\\\n",
    "              % (epoch+1, EPOCHS, train_loss_[epoch], test_loss_[epoch], test_bal_loss_[epoch],\n",
    "                 train_acc_[epoch], test_acc_[epoch], test_bal_acc_[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trans_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get just the last epoch\n",
    "train_predictions = train_predictions_tmp[int(-len(train_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_train_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_predictions)):\n",
    "    d = torch.softmax(train_predictions[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_train_predictions.append(e[1])\n",
    "unpacked_train_predictions = np.array(unpacked_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(train_y, unpacked_train_predictions)\n",
    "auc_train = metrics.auc(fpr, tpr)\n",
    "print(auc_train)\n",
    "PRF1 = metrics.precision_recall_fscore_support(train_y, unpacked_train_predictions.round(), average='binary')[:3]    \n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_predictions_tmp[int(-len(test_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_test_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_predictions)):\n",
    "    d = torch.softmax(test_predictions[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_test_predictions.append(e[1])\n",
    "unpacked_test_predictions = np.array(unpacked_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(test_y, unpacked_test_predictions)\n",
    "auc_test = metrics.auc(fpr, tpr)\n",
    "PRF1 = metrics.precision_recall_fscore_support(test_y, unpacked_test_predictions.round(), average='binary')[:3]    \n",
    "print(auc_test)\n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(unpacked_test_predictions.round() >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_bal = test_bal_predictions_tmp[int(-len(test_bal_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_test_predictions_bal = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_predictions_bal)):\n",
    "    d = torch.softmax(test_predictions_bal[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_test_predictions_bal.append(e[1])\n",
    "unpacked_test_predictions_bal = np.array(unpacked_test_predictions_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(test_y_bal, unpacked_test_predictions_bal)\n",
    "auc_test_bal = metrics.auc(fpr, tpr)\n",
    "PRF1 = metrics.precision_recall_fscore_support(test_y_bal, unpacked_test_predictions_bal.round(), average='binary')[:3]    \n",
    "print(auc_test_bal)\n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.describe(unpacked_train_predictions))\n",
    "print(stats.describe(unpacked_test_predictions))\n",
    "print(stats.describe(unpacked_test_predictions_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_ = np.array([x.numpy() for x in train_acc_])\n",
    "test_acc_ = np.array([x.numpy() for x in test_acc_])\n",
    "test_bal_acc_ = np.array([x.numpy() for x in test_bal_acc_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ = np.array(train_loss_)\n",
    "test_loss_ = np.array(test_loss_)\n",
    "test_bal_loss_ = np.array(test_bal_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bal_loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
