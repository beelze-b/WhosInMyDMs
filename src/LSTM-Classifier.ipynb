{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.005\n",
    "HIDDEN_DIM = 5\n",
    "GRADIENT_CLIP = 10\n",
    "DROPOUT_PARAM = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(TwoGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/SMSSpamCollection', sep='\\t', header=None)\n",
    "data['Text'] = data[1].str.replace('[^\\w\\s]','')\n",
    "data.columns = ['label', 'Full Text', 'Text']\n",
    "data['Lower Case Text'] = data['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(data['label'], return_counts=True)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(labels[np.argsort(-counts)])\n",
    "data['y'] = encoder.transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mask_train = np.random.random(data.shape[0]) < 0.8\n",
    "data_train = data[mask_train]\n",
    "data_test = data.iloc[~mask_train, :]\n",
    "\n",
    "\n",
    "#up sample data train for word2vec vocabulary, this must be done exactly was it was done for word2vec training\n",
    "countToIncrease = data_train[data_train['y'] == 0].shape[0] - data_train[data_train['y'] == 1].shape[0]\n",
    "spamupsampled = data_train[data_train['y'] == 1].sample(n=countToIncrease, replace=True)\n",
    "data_train_upsample_word2vec = pd.concat([spamupsampled, data_train]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "count_vect_sing_word = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "count_vect_sing_word.fit(data_train_upsample_word2vec['Lower Case Text'])\n",
    "tokenizer_word = count_vect_sing_word.build_tokenizer()\n",
    "\n",
    "#up sampling specificiallly to hve blanced data set and also match batch size\n",
    "#countToIncrease_word = data_train[data_train['y'] == 0].shape[0]\n",
    "#while (countToIncrease_word  +  data_train[data_train['y'] == 0].shape[0]) % BATCH_SIZE != 0:\n",
    "    #countToIncrease_word = countToIncrease_word + 1\n",
    "#spamupsampled_word = data_train[data_train['y'] == 1].sample(n=countToIncrease_word, replace=True)\n",
    "#data_train_upsample_word2vec = pd.concat([spamupsampled_word, data_train[data_train['y'] == 0]])\\\n",
    "                               #.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "countToIncrease_word = data_train[data_train['y'] == 1].shape[0]\n",
    "while (countToIncrease_word  +  data_train[data_train['y'] == 0].shape[0]) % BATCH_SIZE != 0:\n",
    "    countToIncrease_word = countToIncrease_word + 1\n",
    "spamupsampled_word = data_train[data_train['y'] == 1].sample(n=countToIncrease_word, replace=True)\n",
    "data_train_upsample_word2vec = pd.concat([spamupsampled_word, data_train[data_train['y'] == 0]])\\\n",
    "                               .sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = len(data_test) % BATCH_SIZE\n",
    "data_test_spam = data_test[data_test['y'] == 1]\n",
    "\n",
    "data_test_downsample_ham = data_test[data_test['y'] == 0].sample(len(data_test[data_test['y'] == 0]) - diff)\n",
    "data_test_downsample = pd.concat([data_test_spam, data_test_downsample_ham]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data down sample balanced\n",
    "## won't work for general batch_sizes\n",
    "test_spam_count = data_test[data_test['y'] == 1].shape[0]\n",
    "diff = test_spam_count % BATCH_SIZE \n",
    "test_spam_count = test_spam_count - diff\n",
    "data_test_downsample_spam = data_test[data_test['y'] == 1].sample(test_spam_count)\n",
    "\n",
    "data_test_downsample_ham = data_test[data_test['y'] == 0].sample(test_spam_count)\n",
    "data_test_downsample_bal = pd.concat([data_test_downsample_ham, data_test_downsample_spam]) \\\n",
    "    .sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "CONTEXT_SIZE = 1\n",
    "VOCAB_SIZE = len(count_vect_sing_word.vocabulary_)\n",
    "word_to_ix = count_vect_sing_word.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = TwoGramLanguageModeler(VOCAB_SIZE, EMBEDDING_SIZE, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_WORD = '../data/word_2vec_model'\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "word2vec_model.load_state_dict(torch.load(MODEL_PATH_WORD))\n",
    "word2vec_model.eval()\n",
    "\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "# TO FIX EMBEDDINGS\n",
    "word_embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect_sing_word is a CountVectorizer\n",
    "def _indicesForSentence(input_str, tokenizer = tokenizer_word, count_vect = count_vect_sing_word):\n",
    "    input_str = list(filter(lambda x: x in count_vect.vocabulary_, tokenizer(input_str)))\n",
    "    return torch.tensor([[word_to_ix[word]] for word in input_str], dtype=torch.long)\n",
    "\n",
    "def sentenceToNumpyInstance(input_str, embedder):\n",
    "    embeddings = embedder(_indicesForSentence(input_str))\n",
    "    if embeddings.shape == torch.Size([0]):\n",
    "        return np.zeros(EMBEDDING_SIZE)\n",
    "    else:\n",
    "        return torch.Tensor.numpy(embeddings.detach())\n",
    "    \n",
    "def word2vec_transform(data, embeddings, field = 'Lower Case Text'):\n",
    "    return np.array(data[field].apply(sentenceToNumpyInstance, embedder=embeddings).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data = word2vec_transform(data_train_upsample_word2vec, embeddings=word_embeddings)\n",
    "trans_test_data = word2vec_transform(data_test_downsample, embeddings=word_embeddings)\n",
    "trans_test_data_bal = word2vec_transform(data_test_downsample_bal, embeddings=word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentenceLengths(data):\n",
    "    sentence_lengths= []\n",
    "    for i in range(len(data)):\n",
    "        e = data[i]\n",
    "        if len(e.shape) > 1:\n",
    "            sentence_lengths.append(e.shape[0])\n",
    "        else:\n",
    "            sentence_lengths.append(1)\n",
    "    return sentence_lengths\n",
    "\n",
    "# order the batch sequences from lengtiest sentence to shortest\n",
    "sentence_lengths = np.array(generateSentenceLengths(trans_data))\n",
    "indices_rv = np.argsort(-sentence_lengths)\n",
    "sentence_lengths = sentence_lengths[indices_rv]\n",
    "trans_data = trans_data[indices_rv]\n",
    "\n",
    "sentence_lengths_test = np.array(generateSentenceLengths(trans_test_data))\n",
    "indices_rv_test = np.argsort(-sentence_lengths_test)\n",
    "sentence_lengths_test = sentence_lengths_test[indices_rv_test]\n",
    "trans_test_data = trans_test_data[indices_rv_test]\n",
    "\n",
    "sentence_lengths_test_bal = np.array(generateSentenceLengths(trans_test_data_bal))\n",
    "indices_rv_test = np.argsort(-sentence_lengths_test_bal)\n",
    "sentence_lengths_test_bal = sentence_lengths_test_bal[indices_rv_test]\n",
    "trans_test_data_bal = trans_test_data_bal[indices_rv_test]\n",
    "\n",
    "\n",
    "assert sentence_lengths[0] >= sentence_lengths_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_so_far = -1\n",
    "for i in range(len(trans_data)):\n",
    "    e = trans_data[i]\n",
    "    if e.shape[0] >= max_len_so_far and len(e.shape) > 1:\n",
    "        max_len_so_far = e.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LEN = max_len_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = LEARNING_RATE * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim = EMBEDDING_SIZE, hidden_dim = HIDDEN_DIM, \\\n",
    "                 label_size = 2, batch_size = BATCH_SIZE, num_layers = 1):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, dropout = DROPOUT_PARAM)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def forward(self, embeds, sentence_lengths):\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, sentence_lengths, batch_first=True)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        #lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        #y = self.hidden2label(lstm_out.float()[:, -1, :])\n",
    "        \n",
    "        # use h_t and last layer\n",
    "        y = self.hidden2label(self.hidden[0][-1].float())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSentences(data):\n",
    "    trans_data_reshape = np.zeros((data.shape[0], SENTENCE_LEN, EMBEDDING_SIZE))\n",
    "    # this will also do padding\n",
    "    for i in range(data.shape[0]):\n",
    "        e = data[i]\n",
    "        if len(e.shape) > 1:\n",
    "            sentence_len_sofar = e.shape[0]\n",
    "            for j in range(sentence_len_sofar):\n",
    "                trans_data_reshape[i, j] = e[j][0]\n",
    "    return trans_data_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data = padSentences(trans_data)\n",
    "trans_test_data = padSentences(trans_test_data)\n",
    "trans_test_data_bal = padSentences(trans_test_data_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data, sen_len):\n",
    "    return model(torch.tensor(data,dtype=torch.float), sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = data_train_upsample_word2vec['y']\n",
    "test_y = data_test_downsample['y']\n",
    "test_y_bal = data_test_downsample_bal['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.CrossEntropyLoss()\n",
    "weight_0c=1.0/(data_train_upsample_word2vec['y'] == 0).sum()\n",
    "weight_1c=1.0/(data_train_upsample_word2vec['y'] == 1).sum()\n",
    "\n",
    "weights = torch.tensor([weight_0c, weight_1c])\n",
    "loss_function = nn.CrossEntropyLoss(weight = weights)\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "test_bal_loss_ = []\n",
    "train_acc_ = []\n",
    "test_acc_ = []\n",
    "test_bal_acc_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017035775127768314"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002575328354365182"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_0c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_LSTM = '../data/LSTM_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[Epoch:   1/ 50] Training Loss: 0.069044, Testing Loss: 0.068607, Test Balanced Loss: 0.077581, Training Acc: 0.693960, Testing Acc: 0.765455, Testing Balanced Acc: 0.478125\n",
      "1\n",
      "[Epoch:   2/ 50] Training Loss: 0.066869, Testing Loss: 0.069073, Test Balanced Loss: 0.083056, Training Acc: 0.825056, Testing Acc: 0.809091, Testing Balanced Acc: 0.484375\n",
      "2\n",
      "[Epoch:   3/ 50] Training Loss: 0.064965, Testing Loss: 0.069881, Test Balanced Loss: 0.086964, Training Acc: 0.839150, Testing Acc: 0.767273, Testing Balanced Acc: 0.465625\n",
      "3\n",
      "[Epoch:   4/ 50] Training Loss: 0.061259, Testing Loss: 0.071188, Test Balanced Loss: 0.092646, Training Acc: 0.827740, Testing Acc: 0.727273, Testing Balanced Acc: 0.462500\n",
      "4\n",
      "[Epoch:   5/ 50] Training Loss: 0.056121, Testing Loss: 0.075033, Test Balanced Loss: 0.102603, Training Acc: 0.819016, Testing Acc: 0.718182, Testing Balanced Acc: 0.456250\n",
      "5\n",
      "[Epoch:   6/ 50] Training Loss: 0.050407, Testing Loss: 0.077813, Test Balanced Loss: 0.111154, Training Acc: 0.823937, Testing Acc: 0.708182, Testing Balanced Acc: 0.484375\n",
      "6\n",
      "[Epoch:   7/ 50] Training Loss: 0.044760, Testing Loss: 0.083838, Test Balanced Loss: 0.128742, Training Acc: 0.841611, Testing Acc: 0.700909, Testing Balanced Acc: 0.478125\n",
      "7\n",
      "[Epoch:   8/ 50] Training Loss: 0.040491, Testing Loss: 0.088285, Test Balanced Loss: 0.136254, Training Acc: 0.855705, Testing Acc: 0.700000, Testing Balanced Acc: 0.484375\n",
      "8\n",
      "[Epoch:   9/ 50] Training Loss: 0.036426, Testing Loss: 0.095135, Test Balanced Loss: 0.149338, Training Acc: 0.872483, Testing Acc: 0.696364, Testing Balanced Acc: 0.475000\n",
      "9\n",
      "[Epoch:  10/ 50] Training Loss: 0.033079, Testing Loss: 0.098779, Test Balanced Loss: 0.156735, Training Acc: 0.887696, Testing Acc: 0.698182, Testing Balanced Acc: 0.496875\n",
      "10\n",
      "[Epoch:  11/ 50] Training Loss: 0.030773, Testing Loss: 0.101226, Test Balanced Loss: 0.161866, Training Acc: 0.892394, Testing Acc: 0.697273, Testing Balanced Acc: 0.493750\n",
      "11\n",
      "[Epoch:  12/ 50] Training Loss: 0.028509, Testing Loss: 0.107407, Test Balanced Loss: 0.173944, Training Acc: 0.898210, Testing Acc: 0.704545, Testing Balanced Acc: 0.493750\n",
      "12\n",
      "[Epoch:  13/ 50] Training Loss: 0.026716, Testing Loss: 0.114766, Test Balanced Loss: 0.192564, Training Acc: 0.907159, Testing Acc: 0.723636, Testing Balanced Acc: 0.487500\n",
      "13\n",
      "[Epoch:  14/ 50] Training Loss: 0.025620, Testing Loss: 0.116884, Test Balanced Loss: 0.190804, Training Acc: 0.916555, Testing Acc: 0.715455, Testing Balanced Acc: 0.487500\n",
      "14\n",
      "[Epoch:  15/ 50] Training Loss: 0.024177, Testing Loss: 0.119608, Test Balanced Loss: 0.196211, Training Acc: 0.915436, Testing Acc: 0.725455, Testing Balanced Acc: 0.493750\n",
      "15\n",
      "[Epoch:  16/ 50] Training Loss: 0.023631, Testing Loss: 0.120528, Test Balanced Loss: 0.200399, Training Acc: 0.919239, Testing Acc: 0.722727, Testing Balanced Acc: 0.478125\n",
      "16\n",
      "[Epoch:  17/ 50] Training Loss: 0.022411, Testing Loss: 0.118174, Test Balanced Loss: 0.192088, Training Acc: 0.925280, Testing Acc: 0.708182, Testing Balanced Acc: 0.493750\n",
      "17\n",
      "[Epoch:  18/ 50] Training Loss: 0.021530, Testing Loss: 0.123647, Test Balanced Loss: 0.201453, Training Acc: 0.926846, Testing Acc: 0.719091, Testing Balanced Acc: 0.471875\n",
      "18\n",
      "[Epoch:  19/ 50] Training Loss: 0.021112, Testing Loss: 0.124031, Test Balanced Loss: 0.213174, Training Acc: 0.926846, Testing Acc: 0.720000, Testing Balanced Acc: 0.471875\n",
      "19\n",
      "[Epoch:  20/ 50] Training Loss: 0.020612, Testing Loss: 0.125711, Test Balanced Loss: 0.202497, Training Acc: 0.932662, Testing Acc: 0.712727, Testing Balanced Acc: 0.493750\n",
      "20\n",
      "[Epoch:  21/ 50] Training Loss: 0.020009, Testing Loss: 0.129741, Test Balanced Loss: 0.216546, Training Acc: 0.932662, Testing Acc: 0.731818, Testing Balanced Acc: 0.475000\n",
      "21\n",
      "[Epoch:  22/ 50] Training Loss: 0.019418, Testing Loss: 0.131438, Test Balanced Loss: 0.214030, Training Acc: 0.931767, Testing Acc: 0.720909, Testing Balanced Acc: 0.468750\n",
      "22\n",
      "[Epoch:  23/ 50] Training Loss: 0.019196, Testing Loss: 0.131032, Test Balanced Loss: 0.214686, Training Acc: 0.938031, Testing Acc: 0.722727, Testing Balanced Acc: 0.481250\n",
      "23\n",
      "[Epoch:  24/ 50] Training Loss: 0.018741, Testing Loss: 0.134785, Test Balanced Loss: 0.228137, Training Acc: 0.939597, Testing Acc: 0.723636, Testing Balanced Acc: 0.487500\n",
      "24\n",
      "[Epoch:  25/ 50] Training Loss: 0.018680, Testing Loss: 0.132508, Test Balanced Loss: 0.223153, Training Acc: 0.941611, Testing Acc: 0.728182, Testing Balanced Acc: 0.468750\n",
      "25\n",
      "[Epoch:  26/ 50] Training Loss: 0.018144, Testing Loss: 0.130810, Test Balanced Loss: 0.216595, Training Acc: 0.942282, Testing Acc: 0.719091, Testing Balanced Acc: 0.478125\n",
      "26\n",
      "[Epoch:  27/ 50] Training Loss: 0.018349, Testing Loss: 0.133663, Test Balanced Loss: 0.225619, Training Acc: 0.938926, Testing Acc: 0.730909, Testing Balanced Acc: 0.493750\n",
      "27\n",
      "[Epoch:  28/ 50] Training Loss: 0.017735, Testing Loss: 0.132927, Test Balanced Loss: 0.228507, Training Acc: 0.945190, Testing Acc: 0.716364, Testing Balanced Acc: 0.478125\n",
      "28\n",
      "[Epoch:  29/ 50] Training Loss: 0.017742, Testing Loss: 0.130170, Test Balanced Loss: 0.214925, Training Acc: 0.944519, Testing Acc: 0.723636, Testing Balanced Acc: 0.481250\n",
      "29\n",
      "[Epoch:  30/ 50] Training Loss: 0.017387, Testing Loss: 0.138232, Test Balanced Loss: 0.227924, Training Acc: 0.947204, Testing Acc: 0.727273, Testing Balanced Acc: 0.478125\n",
      "30\n",
      "[Epoch:  31/ 50] Training Loss: 0.017368, Testing Loss: 0.134012, Test Balanced Loss: 0.230082, Training Acc: 0.944519, Testing Acc: 0.724545, Testing Balanced Acc: 0.481250\n",
      "31\n",
      "[Epoch:  32/ 50] Training Loss: 0.017176, Testing Loss: 0.132334, Test Balanced Loss: 0.214567, Training Acc: 0.948322, Testing Acc: 0.720909, Testing Balanced Acc: 0.475000\n",
      "32\n",
      "[Epoch:  33/ 50] Training Loss: 0.017086, Testing Loss: 0.134825, Test Balanced Loss: 0.225895, Training Acc: 0.945414, Testing Acc: 0.739091, Testing Balanced Acc: 0.490625\n",
      "33\n",
      "[Epoch:  34/ 50] Training Loss: 0.016919, Testing Loss: 0.137965, Test Balanced Loss: 0.236089, Training Acc: 0.948322, Testing Acc: 0.736364, Testing Balanced Acc: 0.471875\n",
      "34\n",
      "[Epoch:  35/ 50] Training Loss: 0.016775, Testing Loss: 0.134213, Test Balanced Loss: 0.217563, Training Acc: 0.950336, Testing Acc: 0.719091, Testing Balanced Acc: 0.478125\n",
      "35\n",
      "[Epoch:  36/ 50] Training Loss: 0.016908, Testing Loss: 0.134096, Test Balanced Loss: 0.213648, Training Acc: 0.950112, Testing Acc: 0.713636, Testing Balanced Acc: 0.471875\n",
      "36\n",
      "[Epoch:  37/ 50] Training Loss: 0.016675, Testing Loss: 0.137549, Test Balanced Loss: 0.228365, Training Acc: 0.948993, Testing Acc: 0.734545, Testing Balanced Acc: 0.481250\n",
      "37\n",
      "[Epoch:  38/ 50] Training Loss: 0.016862, Testing Loss: 0.135401, Test Balanced Loss: 0.220222, Training Acc: 0.948322, Testing Acc: 0.724545, Testing Balanced Acc: 0.481250\n",
      "38\n",
      "[Epoch:  39/ 50] Training Loss: 0.016597, Testing Loss: 0.136263, Test Balanced Loss: 0.221592, Training Acc: 0.951007, Testing Acc: 0.728182, Testing Balanced Acc: 0.487500\n",
      "39\n",
      "[Epoch:  40/ 50] Training Loss: 0.016300, Testing Loss: 0.143567, Test Balanced Loss: 0.246187, Training Acc: 0.953020, Testing Acc: 0.743636, Testing Balanced Acc: 0.471875\n",
      "40\n",
      "[Epoch:  41/ 50] Training Loss: 0.016523, Testing Loss: 0.146034, Test Balanced Loss: 0.239552, Training Acc: 0.953915, Testing Acc: 0.736364, Testing Balanced Acc: 0.484375\n",
      "41\n",
      "[Epoch:  42/ 50] Training Loss: 0.016765, Testing Loss: 0.139307, Test Balanced Loss: 0.227487, Training Acc: 0.949664, Testing Acc: 0.730000, Testing Balanced Acc: 0.493750\n",
      "42\n",
      "[Epoch:  43/ 50] Training Loss: 0.016322, Testing Loss: 0.140908, Test Balanced Loss: 0.231377, Training Acc: 0.951454, Testing Acc: 0.732727, Testing Balanced Acc: 0.496875\n",
      "43\n",
      "[Epoch:  44/ 50] Training Loss: 0.016509, Testing Loss: 0.139859, Test Balanced Loss: 0.225192, Training Acc: 0.946756, Testing Acc: 0.734545, Testing Balanced Acc: 0.490625\n",
      "44\n",
      "[Epoch:  45/ 50] Training Loss: 0.016503, Testing Loss: 0.142682, Test Balanced Loss: 0.229633, Training Acc: 0.955034, Testing Acc: 0.740000, Testing Balanced Acc: 0.490625\n",
      "45\n",
      "[Epoch:  46/ 50] Training Loss: 0.016079, Testing Loss: 0.141537, Test Balanced Loss: 0.231114, Training Acc: 0.950559, Testing Acc: 0.730909, Testing Balanced Acc: 0.500000\n",
      "46\n",
      "[Epoch:  47/ 50] Training Loss: 0.016223, Testing Loss: 0.142753, Test Balanced Loss: 0.239079, Training Acc: 0.952573, Testing Acc: 0.742727, Testing Balanced Acc: 0.484375\n",
      "47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  48/ 50] Training Loss: 0.016221, Testing Loss: 0.140237, Test Balanced Loss: 0.223377, Training Acc: 0.951902, Testing Acc: 0.726364, Testing Balanced Acc: 0.506250\n",
      "48\n",
      "[Epoch:  49/ 50] Training Loss: 0.016054, Testing Loss: 0.138350, Test Balanced Loss: 0.231100, Training Acc: 0.949217, Testing Acc: 0.728182, Testing Balanced Acc: 0.490625\n",
      "49\n",
      "[Epoch:  50/ 50] Training Loss: 0.015925, Testing Loss: 0.138906, Test Balanced Loss: 0.223484, Training Acc: 0.951902, Testing Acc: 0.729091, Testing Balanced Acc: 0.496875\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "train_predictions_tmp = []\n",
    "test_predictions_tmp = []\n",
    "test_bal_predictions_tmp = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print(epoch)\n",
    "    # using adam so might be too much\n",
    "    #optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "    \n",
    "    ## TRAIN TO TRAIN\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = list(chunks(range(trans_data.shape[0]), BATCH_SIZE))\n",
    "    batch_indices = np.array(batch_indices)\n",
    "    np.random.shuffle(batch_indices)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "\n",
    "        inner_data = Variable(torch.tensor(trans_data[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(train_y.iloc[this_batch].values))\n",
    "        #inner_y.weight.requires_grad = False\n",
    "        batch_lengths = sentence_lengths[this_batch]\n",
    "        model.zero_grad()\n",
    "        \n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        \n",
    "        \n",
    "        optimizer.step()        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    train_loss_.append(1.0 * total_loss / total)\n",
    "    train_acc_.append(1.0 * total_acc.float() / total)\n",
    "    \n",
    "    #### TRAIN WITHOUT OPTIMIZE JUST TO GET PREDICTIONS IN SAME ORDER\n",
    "    batch_indices = chunks(range(trans_data.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "\n",
    "        inner_data = Variable(torch.tensor(trans_data[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(train_y.iloc[this_batch].values))\n",
    "        batch_lengths = sentence_lengths[this_batch]\n",
    "        model.zero_grad()\n",
    "        \n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        output = prediction(inner_data, batch_lengths)        \n",
    "        train_predictions_tmp.append(output.data)\n",
    "    \n",
    "    ### TEST\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_test_data.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_test_data[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(test_y.iloc[this_batch].values))\n",
    "        batch_lengths = sentence_lengths_test[this_batch]\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        test_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    test_loss_.append(total_loss / total)\n",
    "    test_acc_.append(total_acc.float() / total)\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_test_data_bal.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_test_data_bal[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(test_y_bal.iloc[this_batch].values))\n",
    "        batch_lengths = sentence_lengths_test_bal[this_batch]\n",
    "\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        test_bal_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    test_bal_loss_.append(total_loss / total)\n",
    "    test_bal_acc_.append(total_acc.float() / total)\n",
    "\n",
    "    print(('[Epoch: %3d/%3d] Training Loss: %.6f, Testing Loss: %.6f, Test Balanced Loss: %.6f, '+\n",
    "          'Training Acc: %.6f, Testing Acc: %.6f, Testing Balanced Acc: %.6f')\\\n",
    "              % (epoch+1, EPOCHS, train_loss_[epoch], test_loss_[epoch], test_bal_loss_[epoch],\n",
    "                 train_acc_[epoch], test_acc_[epoch], test_bal_acc_[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get just the last epoch\n",
    "train_predictions = train_predictions_tmp[int(-len(train_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_train_predictions = []\n",
    "unpacked_train_predicted = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_predictions)):\n",
    "    d = torch.softmax(train_predictions[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_train_predictions.append(e[1])\n",
    "unpacked_train_predictions = np.array(unpacked_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9908227055337971\n",
      "(0.7490144546649146, 0.9710391822827938, 0.8456973293768545)\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(train_y, unpacked_train_predictions)\n",
    "auc_train = metrics.auc(fpr, tpr)\n",
    "print(auc_train)\n",
    "PRF1 = metrics.precision_recall_fscore_support(train_y, unpacked_train_predictions.round(), average='binary')[:3]    \n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9534675615212528"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_y == unpacked_train_predictions.round()).sum() / len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f39104d3b70>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_predictions_tmp[int(-len(test_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_test_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_predictions)):\n",
    "    d = torch.softmax(test_predictions[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_test_predictions.append(e[1])\n",
    "unpacked_test_predictions = np.array(unpacked_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7290909090909091"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_y == unpacked_test_predictions.round()).sum() / len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5052275483088672\n",
      "(0.15625, 0.18072289156626506, 0.16759776536312848)\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(test_y, unpacked_test_predictions)\n",
    "auc_test = metrics.auc(fpr, tpr)\n",
    "PRF1 = metrics.precision_recall_fscore_support(test_y, unpacked_test_predictions.round(), average='binary')[:3]    \n",
    "print(auc_test)\n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f391046f048>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_bal = test_bal_predictions_tmp[int(-len(test_bal_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_test_predictions_bal = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_predictions_bal)):\n",
    "    d = torch.softmax(test_predictions_bal[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_test_predictions_bal.append(e[1])\n",
    "unpacked_test_predictions_bal = np.array(unpacked_test_predictions_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5195703125000001\n",
      "(0.49122807017543857, 0.175, 0.2580645161290322)\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(test_y_bal, unpacked_test_predictions_bal)\n",
    "auc_test_bal = metrics.auc(fpr, tpr)\n",
    "PRF1 = metrics.precision_recall_fscore_support(test_y_bal, unpacked_test_predictions_bal.round(), average='binary')[:3]    \n",
    "print(auc_test_bal)\n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=4470, minmax=(5.1524843e-05, 0.99978846), mean=0.21664375, variance=0.09945742, skewness=1.5599913597106934, kurtosis=0.8593915728694452)\n",
      "DescribeResult(nobs=1100, minmax=(0.00013252927, 0.99418056), mean=0.22413759, variance=0.07956627, skewness=1.3597716093063354, kurtosis=0.6033691605789393)\n",
      "DescribeResult(nobs=320, minmax=(0.00024951427, 0.9816472), mean=0.2191365, variance=0.080991715, skewness=1.3720049858093262, kurtosis=0.5948175694742557)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.describe(unpacked_train_predictions))\n",
    "print(stats.describe(unpacked_test_predictions))\n",
    "print(stats.describe(unpacked_test_predictions_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_ = np.array([x.numpy() for x in train_acc_])\n",
    "test_acc_ = np.array([x.numpy() for x in test_acc_])\n",
    "test_bal_acc_ = np.array([x.numpy() for x in test_bal_acc_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ = np.array(train_loss_)\n",
    "test_loss_ = np.array(test_loss_)\n",
    "test_bal_loss_ = np.array(test_bal_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/train_acc', train_acc_)\n",
    "np.save('../data/test_acc_', test_acc_)\n",
    "np.save('../data/test_bal_acc_', test_bal_acc_)\n",
    "np.save('../data/train_loss_', train_loss_)\n",
    "np.save('../data/test_loss_', test_loss_)\n",
    "np.save('../data/test_bal_loss_', test_bal_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
