{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.001\n",
    "HIDDEN_DIM = 10\n",
    "GRADIENT_CLIP = 10\n",
    "DROPOUT_PARAM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(TwoGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/SMSSpamCollection', sep='\\t', header=None)\n",
    "data['Text'] = data[1].str.replace('[^\\w\\s]','')\n",
    "data.columns = ['label', 'Full Text', 'Text']\n",
    "data['Lower Case Text'] = data['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(data['label'], return_counts=True)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(labels[np.argsort(-counts)])\n",
    "data['y'] = encoder.transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mask_train = np.random.random(data.shape[0]) < 0.8\n",
    "data_train = data[mask_train]\n",
    "data_test = data.iloc[~mask_train, :]\n",
    "\n",
    "\n",
    "#up sample data train for word2vec vocabulary, this must be done exactly was it was done for word2vec training\n",
    "countToIncrease = data_train[data_train['y'] == 0].shape[0] - data_train[data_train['y'] == 1].shape[0]\n",
    "spamupsampled = data_train[data_train['y'] == 1].sample(n=countToIncrease, replace=True)\n",
    "data_train_upsample_word2vec = pd.concat([spamupsampled, data_train]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "count_vect_sing_word = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "count_vect_sing_word.fit(data_train_upsample_word2vec['Lower Case Text'])\n",
    "tokenizer_word = count_vect_sing_word.build_tokenizer()\n",
    "\n",
    "#up sampling specificiallly to hve blanced data set and also match batch size\n",
    "countToIncrease_word = data_train[data_train['y'] == 0].shape[0]\n",
    "while (countToIncrease_word +  data_train[data_train['y'] == 0].shape[0]) % BATCH_SIZE != 0:\n",
    "    countToIncrease_word = countToIncrease_word + 1\n",
    "spamupsampled_word = data_train[data_train['y'] == 1].sample(n=countToIncrease_word, replace=True)\n",
    "data_train_upsample_word2vec = pd.concat([spamupsampled_word, data_train[data_train['y'] == 0]])\\\n",
    "                               .sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = len(data_test) % BATCH_SIZE\n",
    "data_test_spam = data_test[data_test['y'] == 1]\n",
    "\n",
    "data_test_downsample_ham = data_test[data_test['y'] == 0].sample(len(data_test[data_test['y'] == 0]) - diff)\n",
    "data_test_downsample = pd.concat([data_test_spam, data_test_downsample_ham]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data down sample balanced\n",
    "## won't work for general batch_sizes\n",
    "test_spam_count = data_test[data_test['y'] == 1].shape[0]\n",
    "diff = test_spam_count % BATCH_SIZE \n",
    "test_spam_count = test_spam_count - diff\n",
    "data_test_downsample_spam = data_test[data_test['y'] == 1].sample(test_spam_count)\n",
    "\n",
    "data_test_downsample_ham = data_test[data_test['y'] == 0].sample(test_spam_count)\n",
    "data_test_downsample_bal = pd.concat([data_test_downsample_ham, data_test_downsample_spam]) \\\n",
    "    .sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "CONTEXT_SIZE = 1\n",
    "VOCAB_SIZE = len(count_vect_sing_word.vocabulary_)\n",
    "word_to_ix = count_vect_sing_word.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = TwoGramLanguageModeler(VOCAB_SIZE, EMBEDDING_SIZE, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_WORD = '../data/word_2vec_model'\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "word2vec_model.load_state_dict(torch.load(MODEL_PATH_WORD))\n",
    "word2vec_model.eval()\n",
    "\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "# TO FIX EMBEDDINGS\n",
    "word_embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect_sing_word is a CountVectorizer\n",
    "def _indicesForSentence(input_str, tokenizer = tokenizer_word, count_vect = count_vect_sing_word):\n",
    "    input_str = list(filter(lambda x: x in count_vect.vocabulary_, tokenizer(input_str)))\n",
    "    return torch.tensor([[word_to_ix[word]] for word in input_str], dtype=torch.long)\n",
    "\n",
    "def sentenceToNumpyInstance(input_str, embedder):\n",
    "    embeddings = embedder(_indicesForSentence(input_str))\n",
    "    if embeddings.shape == torch.Size([0]):\n",
    "        return np.zeros(EMBEDDING_SIZE)\n",
    "    else:\n",
    "        return torch.Tensor.numpy(embeddings.detach())\n",
    "    \n",
    "def word2vec_transform(data, embeddings, field = 'Lower Case Text'):\n",
    "    return np.array(data[field].apply(sentenceToNumpyInstance, embedder=embeddings).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data = word2vec_transform(data_train_upsample_word2vec, embeddings=word_embeddings)\n",
    "trans_test_data = word2vec_transform(data_test_downsample, embeddings=word_embeddings)\n",
    "trans_test_data_bal = word2vec_transform(data_test_downsample_bal, embeddings=word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentenceLengths(data):\n",
    "    sentence_lengths= []\n",
    "    for i in range(len(data)):\n",
    "        e = data[i]\n",
    "        if len(e.shape) > 1:\n",
    "            sentence_lengths.append(e.shape[0])\n",
    "        else:\n",
    "            sentence_lengths.append(1)\n",
    "    return sentence_lengths\n",
    "\n",
    "sentence_lengths = np.array(generateSentenceLengths(trans_data))\n",
    "indices_rv = np.argsort(-sentence_lengths)\n",
    "sentence_lengths = sentence_lengths[indices_rv]\n",
    "trans_data = trans_data[indices_rv]\n",
    "\n",
    "sentence_lengths_test = np.array(generateSentenceLengths(trans_test_data))\n",
    "indices_rv_test = np.argsort(-sentence_lengths_test)\n",
    "sentence_lengths_test = sentence_lengths_test[indices_rv_test]\n",
    "trans_test_data = trans_test_data[indices_rv_test]\n",
    "\n",
    "sentence_lengths_test_bal = np.array(generateSentenceLengths(trans_test_data_bal))\n",
    "indices_rv_test = np.argsort(-sentence_lengths_test_bal)\n",
    "sentence_lengths_test_bal = sentence_lengths_test_bal[indices_rv_test]\n",
    "trans_test_data_bal = trans_test_data_bal[indices_rv_test]\n",
    "\n",
    "\n",
    "assert sentence_lengths[0] >= sentence_lengths_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_so_far = -1\n",
    "for i in range(len(trans_data)):\n",
    "    e = trans_data[i]\n",
    "    if e.shape[0] >= max_len_so_far and len(e.shape) > 1:\n",
    "        max_len_so_far = e.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LEN = max_len_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = LEARNING_RATE * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim = EMBEDDING_SIZE, hidden_dim = HIDDEN_DIM, \\\n",
    "                 label_size = 2, batch_size = BATCH_SIZE, num_layers = 2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, dropout = DROPOUT_PARAM)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def forward(self, embeds, sentence_lengths):\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, sentence_lengths, batch_first=True)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        #lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        #y = self.hidden2label(lstm_out.float()[:, -1, :])\n",
    "        \n",
    "        # use h_t and last layer\n",
    "        y = self.hidden2label(self.hidden[0][-1].float())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSentences(data):\n",
    "    trans_data_reshape = np.zeros((data.shape[0], SENTENCE_LEN, EMBEDDING_SIZE))\n",
    "    # this will also do padding\n",
    "    for i in range(data.shape[0]):\n",
    "        e = data[i]\n",
    "        if len(e.shape) > 1:\n",
    "            sentence_len_sofar = e.shape[0]\n",
    "            for j in range(sentence_len_sofar):\n",
    "                trans_data_reshape[i, j] = e[j][0]\n",
    "    return trans_data_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data = padSentences(trans_data)\n",
    "trans_test_data = padSentences(trans_test_data)\n",
    "trans_test_data_bal = padSentences(trans_test_data_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data, sen_len):\n",
    "    return model(torch.tensor(data,dtype=torch.float), sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = data_train_upsample_word2vec['y']\n",
    "test_y = data_test_downsample['y']\n",
    "test_y_bal = data_test_downsample_bal['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "test_bal_loss_ = []\n",
    "train_acc_ = []\n",
    "test_acc_ = []\n",
    "test_bal_acc_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_LSTM = '../data/LSTM_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[Epoch:   1/ 50] Training Loss: 0.069376, Testing Loss: 0.068565, Test Balanced Loss: 0.069112, Training Acc: 0.504633, Testing Acc: 0.589091, Testing Balanced Acc: 0.543750\n",
      "1\n",
      "[Epoch:   2/ 50] Training Loss: 0.069318, Testing Loss: 0.069616, Test Balanced Loss: 0.069399, Training Acc: 0.506950, Testing Acc: 0.375455, Testing Balanced Acc: 0.490625\n",
      "2\n",
      "[Epoch:   3/ 50] Training Loss: 0.069303, Testing Loss: 0.069324, Test Balanced Loss: 0.069136, Training Acc: 0.511840, Testing Acc: 0.455455, Testing Balanced Acc: 0.546875\n",
      "3\n",
      "[Epoch:   4/ 50] Training Loss: 0.069229, Testing Loss: 0.069099, Test Balanced Loss: 0.069351, Training Acc: 0.519434, Testing Acc: 0.518182, Testing Balanced Acc: 0.500000\n",
      "4\n",
      "[Epoch:   5/ 50] Training Loss: 0.069195, Testing Loss: 0.068060, Test Balanced Loss: 0.069642, Training Acc: 0.527284, Testing Acc: 0.663636, Testing Balanced Acc: 0.487500\n",
      "5\n",
      "[Epoch:   6/ 50] Training Loss: 0.069078, Testing Loss: 0.070032, Test Balanced Loss: 0.069327, Training Acc: 0.530888, Testing Acc: 0.440909, Testing Balanced Acc: 0.518750\n",
      "6\n",
      "[Epoch:   7/ 50] Training Loss: 0.068891, Testing Loss: 0.067918, Test Balanced Loss: 0.069758, Training Acc: 0.542214, Testing Acc: 0.611818, Testing Balanced Acc: 0.484375\n",
      "7\n",
      "[Epoch:   8/ 50] Training Loss: 0.068706, Testing Loss: 0.068644, Test Balanced Loss: 0.069376, Training Acc: 0.543629, Testing Acc: 0.574545, Testing Balanced Acc: 0.521875\n",
      "8\n",
      "[Epoch:   9/ 50] Training Loss: 0.068637, Testing Loss: 0.068011, Test Balanced Loss: 0.069917, Training Acc: 0.551995, Testing Acc: 0.581818, Testing Balanced Acc: 0.484375\n",
      "9\n",
      "[Epoch:  10/ 50] Training Loss: 0.068376, Testing Loss: 0.069633, Test Balanced Loss: 0.069903, Training Acc: 0.554311, Testing Acc: 0.540000, Testing Balanced Acc: 0.534375\n",
      "10\n",
      "[Epoch:  11/ 50] Training Loss: 0.068367, Testing Loss: 0.070304, Test Balanced Loss: 0.068905, Training Acc: 0.558172, Testing Acc: 0.503636, Testing Balanced Acc: 0.578125\n",
      "11\n",
      "[Epoch:  12/ 50] Training Loss: 0.067956, Testing Loss: 0.067795, Test Balanced Loss: 0.070120, Training Acc: 0.566023, Testing Acc: 0.595455, Testing Balanced Acc: 0.543750\n",
      "12\n",
      "[Epoch:  13/ 50] Training Loss: 0.068212, Testing Loss: 0.071101, Test Balanced Loss: 0.070723, Training Acc: 0.563835, Testing Acc: 0.493636, Testing Balanced Acc: 0.475000\n",
      "13\n",
      "[Epoch:  14/ 50] Training Loss: 0.067584, Testing Loss: 0.070144, Test Balanced Loss: 0.070181, Training Acc: 0.579537, Testing Acc: 0.540909, Testing Balanced Acc: 0.512500\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "train_predictions_tmp = []\n",
    "test_predictions_tmp = []\n",
    "test_bal_predictions_tmp = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print(epoch)\n",
    "    # using adam so might be too much\n",
    "    #optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = list(chunks(range(trans_data.shape[0]), BATCH_SIZE))\n",
    "    batch_indices = np.array(batch_indices)\n",
    "    np.random.shuffle(batch_indices)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "\n",
    "        inner_data = Variable(torch.tensor(trans_data[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(train_y.iloc[this_batch].values))\n",
    "        #inner_y.weight.requires_grad = False\n",
    "        batch_lengths = sentence_lengths[this_batch]\n",
    "        model.zero_grad()\n",
    "        \n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        train_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    train_loss_.append(1.0 * total_loss / total)\n",
    "    train_acc_.append(1.0 * total_acc.float() / total)\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_test_data.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_test_data[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(test_y.iloc[this_batch].values))\n",
    "        batch_lengths = sentence_lengths_test[this_batch]\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        test_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    test_loss_.append(total_loss / total)\n",
    "    test_acc_.append(total_acc.float() / total)\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_test_data_bal.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_test_data_bal[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(test_y_bal.iloc[this_batch].values))\n",
    "        batch_lengths = sentence_lengths_test_bal[this_batch]\n",
    "\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        test_bal_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    test_bal_loss_.append(total_loss / total)\n",
    "    test_bal_acc_.append(total_acc.float() / total)\n",
    "\n",
    "    print(('[Epoch: %3d/%3d] Training Loss: %.6f, Testing Loss: %.6f, Test Balanced Loss: %.6f, '+\n",
    "          'Training Acc: %.6f, Testing Acc: %.6f, Testing Balanced Acc: %.6f')\\\n",
    "              % (epoch+1, EPOCHS, train_loss_[epoch], test_loss_[epoch], test_bal_loss_[epoch],\n",
    "                 train_acc_[epoch], test_acc_[epoch], test_bal_acc_[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get just the last epoch\n",
    "train_predictions = train_predictions_tmp[int(-len(train_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_train_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_predictions)):\n",
    "    d = torch.softmax(train_predictions[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_train_predictions.append(e[1])\n",
    "unpacked_train_predictions = np.array(unpacked_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(train_y, unpacked_train_predictions)\n",
    "auc_train = metrics.auc(fpr, tpr)\n",
    "print(auc_train)\n",
    "PRF1 = metrics.precision_recall_fscore_support(train_y, unpacked_train_predictions.round(), average='binary')[:3]    \n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_predictions_tmp[int(-len(test_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_test_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_predictions)):\n",
    "    d = torch.softmax(test_predictions[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_test_predictions.append(e[1])\n",
    "unpacked_test_predictions = np.array(unpacked_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(test_y, unpacked_test_predictions)\n",
    "auc_test = metrics.auc(fpr, tpr)\n",
    "PRF1 = metrics.precision_recall_fscore_support(test_y, unpacked_test_predictions.round(), average='binary')[:3]    \n",
    "print(auc_test)\n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(unpacked_test_predictions.round() >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_bal = test_bal_predictions_tmp[int(-len(test_bal_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_test_predictions_bal = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_predictions_bal)):\n",
    "    d = torch.softmax(test_predictions_bal[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_test_predictions_bal.append(e[1])\n",
    "unpacked_test_predictions_bal = np.array(unpacked_test_predictions_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(test_y_bal, unpacked_test_predictions_bal)\n",
    "auc_test_bal = metrics.auc(fpr, tpr)\n",
    "PRF1 = metrics.precision_recall_fscore_support(test_y_bal, unpacked_test_predictions_bal.round(), average='binary')[:3]    \n",
    "print(auc_test_bal)\n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.describe(unpacked_train_predictions))\n",
    "print(stats.describe(unpacked_test_predictions))\n",
    "print(stats.describe(unpacked_test_predictions_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_ = np.array([x.numpy() for x in train_acc_])\n",
    "test_acc_ = np.array([x.numpy() for x in test_acc_])\n",
    "test_bal_acc_ = np.array([x.numpy() for x in test_bal_acc_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ = np.array(train_loss_)\n",
    "test_loss_ = np.array(test_loss_)\n",
    "test_bal_loss_ = np.array(test_bal_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/train_acc', train_acc_)\n",
    "np.save('../data/test_acc_', test_acc_)\n",
    "np.save('../data/test_bal_acc_', test_bal_acc_)\n",
    "np.save('../data/train_loss_', train_loss_)\n",
    "np.save('../data/test_loss_', test_loss_)\n",
    "np.save('../data/test_bal_loss_', test_bal_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
