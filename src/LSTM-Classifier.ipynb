{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 0.001\n",
    "HIDDEN_DIM = 5\n",
    "GRADIENT_CLIP = 6\n",
    "DROPOUT_PARAM = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(TwoGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/SMSSpamCollection', sep='\\t', header=None)\n",
    "data['Text'] = data[1].str.replace('[^\\w\\s]','')\n",
    "data.columns = ['label', 'Full Text', 'Text']\n",
    "data['Lower Case Text'] = data['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(data['label'], return_counts=True)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(labels[np.argsort(-counts)])\n",
    "data['y'] = encoder.transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mask_train = np.random.random(data.shape[0]) < 0.8\n",
    "data_train = data[mask_train]\n",
    "data_test = data.iloc[~mask_train, :]\n",
    "\n",
    "\n",
    "#up sample data train for word2vec vocabulary, this must be done exactly was it was done for word2vec training\n",
    "countToIncrease = data_train[data_train['y'] == 0].shape[0] - data_train[data_train['y'] == 1].shape[0]\n",
    "spamupsampled = data_train[data_train['y'] == 1].sample(n=countToIncrease, replace=True)\n",
    "data_train_upsample_word2vec = pd.concat([spamupsampled, data_train]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "count_vect_sing_word = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "count_vect_sing_word.fit(data_train_upsample_word2vec['Lower Case Text'])\n",
    "tokenizer_word = count_vect_sing_word.build_tokenizer()\n",
    "\n",
    "#up sampling specificiallly to hve blanced data set and also match batch size\n",
    "countToIncrease_word = data_train[data_train['y'] == 0].shape[0]\n",
    "while (countToIncrease_word +  data_train[data_train['y'] == 0].shape[0]) % BATCH_SIZE != 0:\n",
    "    countToIncrease_word = countToIncrease_word + 1\n",
    "spamupsampled_word = data_train[data_train['y'] == 1].sample(n=countToIncrease_word, replace=True)\n",
    "data_train_upsample_word2vec = pd.concat([spamupsampled_word, data_train[data_train['y'] == 0]])\\\n",
    "                               .sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = len(data_test) % BATCH_SIZE\n",
    "data_test_spam = data_test[data_test['y'] == 1]\n",
    "\n",
    "data_test_downsample_ham = data_test[data_test['y'] == 0].sample(len(data_test[data_test['y'] == 0]) - diff)\n",
    "data_test_downsample = pd.concat([data_test_spam, data_test_downsample_ham]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data down sample balanced\n",
    "## won't work for general batch_sizes\n",
    "test_spam_count = data_test[data_test['y'] == 1].shape[0]\n",
    "diff = test_spam_count % BATCH_SIZE \n",
    "test_spam_count = test_spam_count - diff\n",
    "data_test_downsample_spam = data_test[data_test['y'] == 1].sample(test_spam_count)\n",
    "\n",
    "data_test_downsample_ham = data_test[data_test['y'] == 0].sample(test_spam_count)\n",
    "data_test_downsample_bal = pd.concat([data_test_downsample_ham, data_test_downsample_spam]) \\\n",
    "    .sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "CONTEXT_SIZE = 1\n",
    "VOCAB_SIZE = len(count_vect_sing_word.vocabulary_)\n",
    "word_to_ix = count_vect_sing_word.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = TwoGramLanguageModeler(VOCAB_SIZE, EMBEDDING_SIZE, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_WORD = '../data/word_2vec_model'\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "word2vec_model.load_state_dict(torch.load(MODEL_PATH_WORD))\n",
    "word2vec_model.eval()\n",
    "\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "# TO FIX EMBEDDINGS\n",
    "word_embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect_sing_word is a CountVectorizer\n",
    "def _indicesForSentence(input_str, tokenizer = tokenizer_word, count_vect = count_vect_sing_word):\n",
    "    input_str = list(filter(lambda x: x in count_vect.vocabulary_, tokenizer(input_str)))\n",
    "    return torch.tensor([[word_to_ix[word]] for word in input_str], dtype=torch.long)\n",
    "\n",
    "def sentenceToNumpyInstance(input_str, embedder):\n",
    "    embeddings = embedder(_indicesForSentence(input_str))\n",
    "    if embeddings.shape == torch.Size([0]):\n",
    "        return np.zeros(EMBEDDING_SIZE)\n",
    "    else:\n",
    "        return torch.Tensor.numpy(embeddings.detach())\n",
    "    \n",
    "def word2vec_transform(data, embeddings, field = 'Lower Case Text'):\n",
    "    return np.array(data[field].apply(sentenceToNumpyInstance, embedder=embeddings).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data = word2vec_transform(data_train_upsample_word2vec, embeddings=word_embeddings)\n",
    "trans_test_data = word2vec_transform(data_test_downsample, embeddings=word_embeddings)\n",
    "trans_test_data_bal = word2vec_transform(data_test_downsample_bal, embeddings=word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentenceLengths(data):\n",
    "    sentence_lengths= []\n",
    "    for i in range(len(data)):\n",
    "        e = data[i]\n",
    "        if len(e.shape) > 1:\n",
    "            sentence_lengths.append(e.shape[0])\n",
    "        else:\n",
    "            sentence_lengths.append(1)\n",
    "    return sentence_lengths\n",
    "\n",
    "sentence_lengths = np.array(generateSentenceLengths(trans_data))\n",
    "indices_rv = np.argsort(-sentence_lengths)\n",
    "sentence_lengths = sentence_lengths[indices_rv]\n",
    "trans_data = trans_data[indices_rv]\n",
    "\n",
    "sentence_lengths_test = np.array(generateSentenceLengths(trans_test_data))\n",
    "indices_rv_test = np.argsort(-sentence_lengths_test)\n",
    "sentence_lengths_test = sentence_lengths_test[indices_rv_test]\n",
    "trans_test_data = trans_test_data[indices_rv_test]\n",
    "\n",
    "sentence_lengths_test_bal = np.array(generateSentenceLengths(trans_test_data_bal))\n",
    "indices_rv_test = np.argsort(-sentence_lengths_test_bal)\n",
    "sentence_lengths_test_bal = sentence_lengths_test_bal[indices_rv_test]\n",
    "trans_test_data_bal = trans_test_data_bal[indices_rv_test]\n",
    "\n",
    "\n",
    "assert sentence_lengths[0] >= sentence_lengths_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_so_far = -1\n",
    "for i in range(len(trans_data)):\n",
    "    e = trans_data[i]\n",
    "    if e.shape[0] >= max_len_so_far and len(e.shape) > 1:\n",
    "        max_len_so_far = e.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LEN = max_len_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = LEARNING_RATE * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim = EMBEDDING_SIZE, hidden_dim = HIDDEN_DIM, \\\n",
    "                 label_size = 2, batch_size = BATCH_SIZE, num_layers = 2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, dropout = DROPOUT_PARAM)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def forward(self, embeds, sentence_lengths):\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, sentence_lengths, batch_first=True)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        #lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        #y = self.hidden2label(lstm_out.float()[:, -1, :])\n",
    "        \n",
    "        # use h_t and last layer\n",
    "        y = self.hidden2label(self.hidden[0][-1].float())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSentences(data):\n",
    "    trans_data_reshape = np.zeros((data.shape[0], SENTENCE_LEN, EMBEDDING_SIZE))\n",
    "    # this will also do padding\n",
    "    for i in range(data.shape[0]):\n",
    "        e = data[i]\n",
    "        if len(e.shape) > 1:\n",
    "            sentence_len_sofar = e.shape[0]\n",
    "            for j in range(sentence_len_sofar):\n",
    "                trans_data_reshape[i, j] = e[j][0]\n",
    "    return trans_data_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data = padSentences(trans_data)\n",
    "trans_test_data = padSentences(trans_test_data)\n",
    "trans_test_data_bal = padSentences(trans_test_data_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data, sen_len):\n",
    "    return model(torch.tensor(data,dtype=torch.float), sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = data_train_upsample_word2vec['y']\n",
    "test_y = data_test_downsample['y']\n",
    "test_y_bal = data_test_downsample_bal['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "test_bal_loss_ = []\n",
    "train_acc_ = []\n",
    "test_acc_ = []\n",
    "test_bal_acc_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_LSTM = '../data/LSTM_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[Epoch:   1/100] Training Loss: 0.070297, Testing Loss: 0.063464, Test Balanced Loss: 0.069875, Training Acc: 0.499357, Testing Acc: 0.831818, Testing Balanced Acc: 0.512500\n",
      "1\n",
      "[Epoch:   2/100] Training Loss: 0.069421, Testing Loss: 0.066668, Test Balanced Loss: 0.069465, Training Acc: 0.504505, Testing Acc: 0.674545, Testing Balanced Acc: 0.509375\n",
      "2\n",
      "[Epoch:   3/100] Training Loss: 0.069272, Testing Loss: 0.067764, Test Balanced Loss: 0.069332, Training Acc: 0.518275, Testing Acc: 0.588182, Testing Balanced Acc: 0.493750\n",
      "3\n",
      "[Epoch:   4/100] Training Loss: 0.069193, Testing Loss: 0.068174, Test Balanced Loss: 0.069439, Training Acc: 0.520077, Testing Acc: 0.554545, Testing Balanced Acc: 0.496875\n",
      "4\n",
      "[Epoch:   5/100] Training Loss: 0.069183, Testing Loss: 0.068468, Test Balanced Loss: 0.069403, Training Acc: 0.519048, Testing Acc: 0.535455, Testing Balanced Acc: 0.481250\n",
      "5\n",
      "[Epoch:   6/100] Training Loss: 0.069098, Testing Loss: 0.068443, Test Balanced Loss: 0.069326, Training Acc: 0.520849, Testing Acc: 0.525455, Testing Balanced Acc: 0.487500\n",
      "6\n",
      "[Epoch:   7/100] Training Loss: 0.069080, Testing Loss: 0.068456, Test Balanced Loss: 0.069607, Training Acc: 0.530245, Testing Acc: 0.538182, Testing Balanced Acc: 0.471875\n",
      "7\n",
      "[Epoch:   8/100] Training Loss: 0.069012, Testing Loss: 0.068499, Test Balanced Loss: 0.069477, Training Acc: 0.533076, Testing Acc: 0.528182, Testing Balanced Acc: 0.487500\n",
      "8\n",
      "[Epoch:   9/100] Training Loss: 0.068952, Testing Loss: 0.068698, Test Balanced Loss: 0.069309, Training Acc: 0.536422, Testing Acc: 0.530000, Testing Balanced Acc: 0.478125\n",
      "9\n",
      "[Epoch:  10/100] Training Loss: 0.068891, Testing Loss: 0.068686, Test Balanced Loss: 0.069691, Training Acc: 0.542214, Testing Acc: 0.530000, Testing Balanced Acc: 0.471875\n",
      "10\n",
      "[Epoch:  11/100] Training Loss: 0.068821, Testing Loss: 0.068924, Test Balanced Loss: 0.069551, Training Acc: 0.546589, Testing Acc: 0.513636, Testing Balanced Acc: 0.487500\n",
      "11\n",
      "[Epoch:  12/100] Training Loss: 0.068697, Testing Loss: 0.068901, Test Balanced Loss: 0.069648, Training Acc: 0.553153, Testing Acc: 0.510000, Testing Balanced Acc: 0.484375\n",
      "12\n",
      "[Epoch:  13/100] Training Loss: 0.068679, Testing Loss: 0.069032, Test Balanced Loss: 0.069788, Training Acc: 0.549421, Testing Acc: 0.500909, Testing Balanced Acc: 0.465625\n",
      "13\n",
      "[Epoch:  14/100] Training Loss: 0.068540, Testing Loss: 0.068956, Test Balanced Loss: 0.069385, Training Acc: 0.560489, Testing Acc: 0.527273, Testing Balanced Acc: 0.493750\n",
      "14\n",
      "[Epoch:  15/100] Training Loss: 0.068459, Testing Loss: 0.069039, Test Balanced Loss: 0.069978, Training Acc: 0.560746, Testing Acc: 0.510909, Testing Balanced Acc: 0.450000\n",
      "15\n",
      "[Epoch:  16/100] Training Loss: 0.068294, Testing Loss: 0.069490, Test Balanced Loss: 0.069934, Training Acc: 0.566281, Testing Acc: 0.484545, Testing Balanced Acc: 0.481250\n",
      "16\n",
      "[Epoch:  17/100] Training Loss: 0.068159, Testing Loss: 0.069446, Test Balanced Loss: 0.070131, Training Acc: 0.566538, Testing Acc: 0.495455, Testing Balanced Acc: 0.525000\n",
      "17\n",
      "[Epoch:  18/100] Training Loss: 0.067994, Testing Loss: 0.069663, Test Balanced Loss: 0.070344, Training Acc: 0.573488, Testing Acc: 0.501818, Testing Balanced Acc: 0.496875\n",
      "18\n",
      "[Epoch:  19/100] Training Loss: 0.067839, Testing Loss: 0.069826, Test Balanced Loss: 0.070875, Training Acc: 0.577992, Testing Acc: 0.496364, Testing Balanced Acc: 0.481250\n",
      "19\n",
      "[Epoch:  20/100] Training Loss: 0.067548, Testing Loss: 0.070389, Test Balanced Loss: 0.070497, Training Acc: 0.582883, Testing Acc: 0.476364, Testing Balanced Acc: 0.506250\n",
      "20\n",
      "[Epoch:  21/100] Training Loss: 0.067362, Testing Loss: 0.070307, Test Balanced Loss: 0.070910, Training Acc: 0.589704, Testing Acc: 0.498182, Testing Balanced Acc: 0.509375\n",
      "21\n",
      "[Epoch:  22/100] Training Loss: 0.067240, Testing Loss: 0.070612, Test Balanced Loss: 0.071021, Training Acc: 0.588803, Testing Acc: 0.480000, Testing Balanced Acc: 0.500000\n",
      "22\n",
      "[Epoch:  23/100] Training Loss: 0.067084, Testing Loss: 0.070769, Test Balanced Loss: 0.071067, Training Acc: 0.593436, Testing Acc: 0.494545, Testing Balanced Acc: 0.487500\n",
      "23\n",
      "[Epoch:  24/100] Training Loss: 0.066749, Testing Loss: 0.071252, Test Balanced Loss: 0.071709, Training Acc: 0.595109, Testing Acc: 0.480000, Testing Balanced Acc: 0.512500\n",
      "24\n",
      "[Epoch:  25/100] Training Loss: 0.066615, Testing Loss: 0.070896, Test Balanced Loss: 0.072156, Training Acc: 0.604118, Testing Acc: 0.492727, Testing Balanced Acc: 0.481250\n",
      "25\n",
      "[Epoch:  26/100] Training Loss: 0.066252, Testing Loss: 0.071465, Test Balanced Loss: 0.071964, Training Acc: 0.609653, Testing Acc: 0.490000, Testing Balanced Acc: 0.500000\n",
      "26\n",
      "[Epoch:  27/100] Training Loss: 0.066092, Testing Loss: 0.071480, Test Balanced Loss: 0.072882, Training Acc: 0.607851, Testing Acc: 0.485455, Testing Balanced Acc: 0.487500\n",
      "27\n",
      "[Epoch:  28/100] Training Loss: 0.065893, Testing Loss: 0.071792, Test Balanced Loss: 0.072731, Training Acc: 0.608366, Testing Acc: 0.510909, Testing Balanced Acc: 0.484375\n",
      "28\n",
      "[Epoch:  29/100] Training Loss: 0.065430, Testing Loss: 0.071931, Test Balanced Loss: 0.073339, Training Acc: 0.620592, Testing Acc: 0.510000, Testing Balanced Acc: 0.490625\n",
      "29\n",
      "[Epoch:  30/100] Training Loss: 0.065222, Testing Loss: 0.072376, Test Balanced Loss: 0.073307, Training Acc: 0.614672, Testing Acc: 0.502727, Testing Balanced Acc: 0.503125\n",
      "30\n",
      "[Epoch:  31/100] Training Loss: 0.064981, Testing Loss: 0.072561, Test Balanced Loss: 0.073609, Training Acc: 0.620463, Testing Acc: 0.501818, Testing Balanced Acc: 0.500000\n",
      "31\n",
      "[Epoch:  32/100] Training Loss: 0.064722, Testing Loss: 0.072420, Test Balanced Loss: 0.074472, Training Acc: 0.626512, Testing Acc: 0.518182, Testing Balanced Acc: 0.468750\n",
      "32\n",
      "[Epoch:  33/100] Training Loss: 0.064337, Testing Loss: 0.072789, Test Balanced Loss: 0.073968, Training Acc: 0.626126, Testing Acc: 0.518182, Testing Balanced Acc: 0.493750\n",
      "33\n",
      "[Epoch:  34/100] Training Loss: 0.064346, Testing Loss: 0.072680, Test Balanced Loss: 0.074472, Training Acc: 0.629730, Testing Acc: 0.521818, Testing Balanced Acc: 0.509375\n",
      "34\n",
      "[Epoch:  35/100] Training Loss: 0.064077, Testing Loss: 0.073909, Test Balanced Loss: 0.075161, Training Acc: 0.630888, Testing Acc: 0.505455, Testing Balanced Acc: 0.478125\n",
      "35\n",
      "[Epoch:  36/100] Training Loss: 0.063631, Testing Loss: 0.073664, Test Balanced Loss: 0.074499, Training Acc: 0.638996, Testing Acc: 0.521818, Testing Balanced Acc: 0.496875\n",
      "36\n",
      "[Epoch:  37/100] Training Loss: 0.063467, Testing Loss: 0.074272, Test Balanced Loss: 0.075289, Training Acc: 0.634234, Testing Acc: 0.510000, Testing Balanced Acc: 0.506250\n",
      "37\n",
      "[Epoch:  38/100] Training Loss: 0.063353, Testing Loss: 0.074361, Test Balanced Loss: 0.075332, Training Acc: 0.637194, Testing Acc: 0.528182, Testing Balanced Acc: 0.506250\n",
      "38\n",
      "[Epoch:  39/100] Training Loss: 0.063186, Testing Loss: 0.074686, Test Balanced Loss: 0.076039, Training Acc: 0.642857, Testing Acc: 0.514545, Testing Balanced Acc: 0.478125\n",
      "39\n",
      "[Epoch:  40/100] Training Loss: 0.062793, Testing Loss: 0.074717, Test Balanced Loss: 0.077266, Training Acc: 0.644402, Testing Acc: 0.525455, Testing Balanced Acc: 0.462500\n",
      "40\n",
      "[Epoch:  41/100] Training Loss: 0.062636, Testing Loss: 0.074574, Test Balanced Loss: 0.076527, Training Acc: 0.645431, Testing Acc: 0.520000, Testing Balanced Acc: 0.518750\n",
      "41\n",
      "[Epoch:  42/100] Training Loss: 0.062417, Testing Loss: 0.074795, Test Balanced Loss: 0.077513, Training Acc: 0.648649, Testing Acc: 0.523636, Testing Balanced Acc: 0.490625\n",
      "42\n",
      "[Epoch:  43/100] Training Loss: 0.062492, Testing Loss: 0.075162, Test Balanced Loss: 0.075317, Training Acc: 0.644144, Testing Acc: 0.520000, Testing Balanced Acc: 0.515625\n",
      "43\n",
      "[Epoch:  44/100] Training Loss: 0.062152, Testing Loss: 0.075151, Test Balanced Loss: 0.076768, Training Acc: 0.648777, Testing Acc: 0.520000, Testing Balanced Acc: 0.521875\n",
      "44\n",
      "[Epoch:  45/100] Training Loss: 0.061681, Testing Loss: 0.076087, Test Balanced Loss: 0.077635, Training Acc: 0.656242, Testing Acc: 0.530000, Testing Balanced Acc: 0.496875\n",
      "45\n",
      "[Epoch:  46/100] Training Loss: 0.061776, Testing Loss: 0.076864, Test Balanced Loss: 0.078724, Training Acc: 0.653411, Testing Acc: 0.530000, Testing Balanced Acc: 0.496875\n",
      "46\n",
      "[Epoch:  47/100] Training Loss: 0.061479, Testing Loss: 0.076949, Test Balanced Loss: 0.076675, Training Acc: 0.657400, Testing Acc: 0.503636, Testing Balanced Acc: 0.509375\n",
      "47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  48/100] Training Loss: 0.061497, Testing Loss: 0.076620, Test Balanced Loss: 0.077512, Training Acc: 0.660489, Testing Acc: 0.527273, Testing Balanced Acc: 0.518750\n",
      "48\n",
      "[Epoch:  49/100] Training Loss: 0.061489, Testing Loss: 0.076407, Test Balanced Loss: 0.076954, Training Acc: 0.656499, Testing Acc: 0.518182, Testing Balanced Acc: 0.493750\n",
      "49\n",
      "[Epoch:  50/100] Training Loss: 0.061144, Testing Loss: 0.078019, Test Balanced Loss: 0.079724, Training Acc: 0.659717, Testing Acc: 0.504545, Testing Balanced Acc: 0.496875\n",
      "50\n",
      "[Epoch:  51/100] Training Loss: 0.060761, Testing Loss: 0.077530, Test Balanced Loss: 0.079991, Training Acc: 0.662420, Testing Acc: 0.509091, Testing Balanced Acc: 0.487500\n",
      "51\n",
      "[Epoch:  52/100] Training Loss: 0.060593, Testing Loss: 0.077876, Test Balanced Loss: 0.078314, Training Acc: 0.664350, Testing Acc: 0.507273, Testing Balanced Acc: 0.490625\n",
      "52\n",
      "[Epoch:  53/100] Training Loss: 0.060823, Testing Loss: 0.077339, Test Balanced Loss: 0.079717, Training Acc: 0.659459, Testing Acc: 0.522727, Testing Balanced Acc: 0.487500\n",
      "53\n",
      "[Epoch:  54/100] Training Loss: 0.060413, Testing Loss: 0.078318, Test Balanced Loss: 0.079470, Training Acc: 0.665508, Testing Acc: 0.514545, Testing Balanced Acc: 0.506250\n",
      "54\n",
      "[Epoch:  55/100] Training Loss: 0.060235, Testing Loss: 0.078570, Test Balanced Loss: 0.079606, Training Acc: 0.669241, Testing Acc: 0.505455, Testing Balanced Acc: 0.503125\n",
      "55\n",
      "[Epoch:  56/100] Training Loss: 0.059787, Testing Loss: 0.078931, Test Balanced Loss: 0.080077, Training Acc: 0.671557, Testing Acc: 0.523636, Testing Balanced Acc: 0.490625\n",
      "56\n",
      "[Epoch:  57/100] Training Loss: 0.059580, Testing Loss: 0.079138, Test Balanced Loss: 0.079731, Training Acc: 0.676319, Testing Acc: 0.520000, Testing Balanced Acc: 0.500000\n",
      "57\n",
      "[Epoch:  58/100] Training Loss: 0.059744, Testing Loss: 0.078857, Test Balanced Loss: 0.079082, Training Acc: 0.675290, Testing Acc: 0.519091, Testing Balanced Acc: 0.512500\n",
      "58\n",
      "[Epoch:  59/100] Training Loss: 0.059558, Testing Loss: 0.080629, Test Balanced Loss: 0.081158, Training Acc: 0.672587, Testing Acc: 0.506364, Testing Balanced Acc: 0.509375\n",
      "59\n",
      "[Epoch:  60/100] Training Loss: 0.059044, Testing Loss: 0.081541, Test Balanced Loss: 0.081673, Training Acc: 0.678250, Testing Acc: 0.506364, Testing Balanced Acc: 0.518750\n",
      "60\n",
      "[Epoch:  61/100] Training Loss: 0.058756, Testing Loss: 0.080619, Test Balanced Loss: 0.081255, Training Acc: 0.679279, Testing Acc: 0.509091, Testing Balanced Acc: 0.503125\n",
      "61\n",
      "[Epoch:  62/100] Training Loss: 0.058727, Testing Loss: 0.081634, Test Balanced Loss: 0.080706, Training Acc: 0.679923, Testing Acc: 0.510909, Testing Balanced Acc: 0.521875\n",
      "62\n",
      "[Epoch:  63/100] Training Loss: 0.059136, Testing Loss: 0.080051, Test Balanced Loss: 0.082160, Training Acc: 0.673616, Testing Acc: 0.534545, Testing Balanced Acc: 0.493750\n",
      "63\n",
      "[Epoch:  64/100] Training Loss: 0.058751, Testing Loss: 0.080667, Test Balanced Loss: 0.084119, Training Acc: 0.674646, Testing Acc: 0.515455, Testing Balanced Acc: 0.468750\n",
      "64\n",
      "[Epoch:  65/100] Training Loss: 0.058583, Testing Loss: 0.081811, Test Balanced Loss: 0.082590, Training Acc: 0.678121, Testing Acc: 0.519091, Testing Balanced Acc: 0.503125\n",
      "65\n",
      "[Epoch:  66/100] Training Loss: 0.058221, Testing Loss: 0.081535, Test Balanced Loss: 0.082303, Training Acc: 0.684427, Testing Acc: 0.520909, Testing Balanced Acc: 0.503125\n",
      "66\n",
      "[Epoch:  67/100] Training Loss: 0.057988, Testing Loss: 0.080836, Test Balanced Loss: 0.084302, Training Acc: 0.683012, Testing Acc: 0.521818, Testing Balanced Acc: 0.515625\n",
      "67\n",
      "[Epoch:  68/100] Training Loss: 0.058178, Testing Loss: 0.081842, Test Balanced Loss: 0.084501, Training Acc: 0.680438, Testing Acc: 0.509091, Testing Balanced Acc: 0.515625\n",
      "68\n",
      "[Epoch:  69/100] Training Loss: 0.057854, Testing Loss: 0.082920, Test Balanced Loss: 0.083877, Training Acc: 0.682754, Testing Acc: 0.507273, Testing Balanced Acc: 0.506250\n",
      "69\n",
      "[Epoch:  70/100] Training Loss: 0.057697, Testing Loss: 0.083535, Test Balanced Loss: 0.083962, Training Acc: 0.691377, Testing Acc: 0.503636, Testing Balanced Acc: 0.509375\n",
      "70\n",
      "[Epoch:  71/100] Training Loss: 0.057298, Testing Loss: 0.082383, Test Balanced Loss: 0.083856, Training Acc: 0.688546, Testing Acc: 0.518182, Testing Balanced Acc: 0.512500\n",
      "71\n",
      "[Epoch:  72/100] Training Loss: 0.057509, Testing Loss: 0.083239, Test Balanced Loss: 0.085267, Training Acc: 0.690219, Testing Acc: 0.512727, Testing Balanced Acc: 0.487500\n",
      "72\n",
      "[Epoch:  73/100] Training Loss: 0.057265, Testing Loss: 0.083747, Test Balanced Loss: 0.083010, Training Acc: 0.693179, Testing Acc: 0.500000, Testing Balanced Acc: 0.506250\n",
      "73\n",
      "[Epoch:  74/100] Training Loss: 0.057069, Testing Loss: 0.084172, Test Balanced Loss: 0.083351, Training Acc: 0.695367, Testing Acc: 0.511818, Testing Balanced Acc: 0.512500\n",
      "74\n",
      "[Epoch:  75/100] Training Loss: 0.057302, Testing Loss: 0.084310, Test Balanced Loss: 0.084815, Training Acc: 0.683269, Testing Acc: 0.500000, Testing Balanced Acc: 0.525000\n",
      "75\n",
      "[Epoch:  76/100] Training Loss: 0.057208, Testing Loss: 0.083431, Test Balanced Loss: 0.088024, Training Acc: 0.692021, Testing Acc: 0.533636, Testing Balanced Acc: 0.484375\n",
      "76\n",
      "[Epoch:  77/100] Training Loss: 0.057366, Testing Loss: 0.085269, Test Balanced Loss: 0.086896, Training Acc: 0.691248, Testing Acc: 0.505455, Testing Balanced Acc: 0.481250\n",
      "77\n",
      "[Epoch:  78/100] Training Loss: 0.056905, Testing Loss: 0.085370, Test Balanced Loss: 0.085315, Training Acc: 0.693436, Testing Acc: 0.509091, Testing Balanced Acc: 0.518750\n",
      "78\n",
      "[Epoch:  79/100] Training Loss: 0.056302, Testing Loss: 0.085269, Test Balanced Loss: 0.088589, Training Acc: 0.698584, Testing Acc: 0.519091, Testing Balanced Acc: 0.500000\n",
      "79\n",
      "[Epoch:  80/100] Training Loss: 0.056403, Testing Loss: 0.085604, Test Balanced Loss: 0.087389, Training Acc: 0.697555, Testing Acc: 0.511818, Testing Balanced Acc: 0.509375\n",
      "80\n",
      "[Epoch:  81/100] Training Loss: 0.056620, Testing Loss: 0.085366, Test Balanced Loss: 0.087730, Training Acc: 0.691377, Testing Acc: 0.519091, Testing Balanced Acc: 0.481250\n",
      "81\n",
      "[Epoch:  82/100] Training Loss: 0.056784, Testing Loss: 0.085965, Test Balanced Loss: 0.086902, Training Acc: 0.687645, Testing Acc: 0.500909, Testing Balanced Acc: 0.518750\n",
      "82\n",
      "[Epoch:  83/100] Training Loss: 0.055714, Testing Loss: 0.086511, Test Balanced Loss: 0.085497, Training Acc: 0.705534, Testing Acc: 0.520909, Testing Balanced Acc: 0.512500\n",
      "83\n",
      "[Epoch:  84/100] Training Loss: 0.056392, Testing Loss: 0.087444, Test Balanced Loss: 0.085915, Training Acc: 0.689704, Testing Acc: 0.501818, Testing Balanced Acc: 0.509375\n",
      "84\n",
      "[Epoch:  85/100] Training Loss: 0.056028, Testing Loss: 0.087301, Test Balanced Loss: 0.085455, Training Acc: 0.696654, Testing Acc: 0.502727, Testing Balanced Acc: 0.525000\n",
      "85\n",
      "[Epoch:  86/100] Training Loss: 0.055722, Testing Loss: 0.087233, Test Balanced Loss: 0.088732, Training Acc: 0.701158, Testing Acc: 0.512727, Testing Balanced Acc: 0.518750\n",
      "86\n",
      "[Epoch:  87/100] Training Loss: 0.055554, Testing Loss: 0.086129, Test Balanced Loss: 0.085501, Training Acc: 0.699485, Testing Acc: 0.510000, Testing Balanced Acc: 0.518750\n",
      "87\n",
      "[Epoch:  88/100] Training Loss: 0.055409, Testing Loss: 0.088109, Test Balanced Loss: 0.086435, Training Acc: 0.703732, Testing Acc: 0.496364, Testing Balanced Acc: 0.521875\n",
      "88\n",
      "[Epoch:  89/100] Training Loss: 0.055412, Testing Loss: 0.086219, Test Balanced Loss: 0.090861, Training Acc: 0.706049, Testing Acc: 0.525455, Testing Balanced Acc: 0.490625\n",
      "89\n",
      "[Epoch:  90/100] Training Loss: 0.056030, Testing Loss: 0.087407, Test Balanced Loss: 0.088261, Training Acc: 0.695495, Testing Acc: 0.488182, Testing Balanced Acc: 0.509375\n",
      "90\n",
      "[Epoch:  91/100] Training Loss: 0.055182, Testing Loss: 0.088637, Test Balanced Loss: 0.091804, Training Acc: 0.704118, Testing Acc: 0.507273, Testing Balanced Acc: 0.512500\n",
      "91\n",
      "[Epoch:  92/100] Training Loss: 0.055056, Testing Loss: 0.086885, Test Balanced Loss: 0.087928, Training Acc: 0.703990, Testing Acc: 0.527273, Testing Balanced Acc: 0.503125\n",
      "92\n",
      "[Epoch:  93/100] Training Loss: 0.054627, Testing Loss: 0.087928, Test Balanced Loss: 0.087494, Training Acc: 0.706178, Testing Acc: 0.508182, Testing Balanced Acc: 0.521875\n",
      "93\n",
      "[Epoch:  94/100] Training Loss: 0.055096, Testing Loss: 0.087342, Test Balanced Loss: 0.090548, Training Acc: 0.699485, Testing Acc: 0.528182, Testing Balanced Acc: 0.506250\n",
      "94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  95/100] Training Loss: 0.054823, Testing Loss: 0.090714, Test Balanced Loss: 0.087020, Training Acc: 0.703732, Testing Acc: 0.485455, Testing Balanced Acc: 0.534375\n",
      "95\n",
      "[Epoch:  96/100] Training Loss: 0.054846, Testing Loss: 0.089215, Test Balanced Loss: 0.093030, Training Acc: 0.704118, Testing Acc: 0.499091, Testing Balanced Acc: 0.503125\n",
      "96\n",
      "[Epoch:  97/100] Training Loss: 0.054536, Testing Loss: 0.090723, Test Balanced Loss: 0.089346, Training Acc: 0.702188, Testing Acc: 0.493636, Testing Balanced Acc: 0.512500\n",
      "97\n",
      "[Epoch:  98/100] Training Loss: 0.054830, Testing Loss: 0.089469, Test Balanced Loss: 0.091590, Training Acc: 0.708365, Testing Acc: 0.510909, Testing Balanced Acc: 0.490625\n",
      "98\n",
      "[Epoch:  99/100] Training Loss: 0.054787, Testing Loss: 0.090038, Test Balanced Loss: 0.091628, Training Acc: 0.704504, Testing Acc: 0.508182, Testing Balanced Acc: 0.496875\n",
      "99\n",
      "[Epoch: 100/100] Training Loss: 0.054307, Testing Loss: 0.087860, Test Balanced Loss: 0.092167, Training Acc: 0.707207, Testing Acc: 0.520909, Testing Balanced Acc: 0.512500\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "train_predictions_tmp = []\n",
    "test_predictions_tmp = []\n",
    "test_bal_predictions_tmp = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print(epoch)\n",
    "    # using adam so might be too much\n",
    "    #optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_data.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_data[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(train_y.iloc[this_batch].values))\n",
    "        #inner_y.weight.requires_grad = False\n",
    "        batch_lengths = sentence_lengths[this_batch]\n",
    "        model.zero_grad()\n",
    "        \n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        train_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    train_loss_.append(1.0 * total_loss / total)\n",
    "    train_acc_.append(1.0 * total_acc.float() / total)\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_test_data.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_test_data[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(test_y.iloc[this_batch].values))\n",
    "        batch_lengths = sentence_lengths_test[this_batch]\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        test_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    test_loss_.append(total_loss / total)\n",
    "    test_acc_.append(total_acc.float() / total)\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_test_data_bal.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_test_data_bal[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(test_y_bal.iloc[this_batch].values))\n",
    "        batch_lengths = sentence_lengths_test_bal[this_batch]\n",
    "\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        test_bal_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    test_bal_loss_.append(total_loss / total)\n",
    "    test_bal_acc_.append(total_acc.float() / total)\n",
    "\n",
    "    print(('[Epoch: %3d/%3d] Training Loss: %.6f, Testing Loss: %.6f, Test Balanced Loss: %.6f, '+\n",
    "          'Training Acc: %.6f, Testing Acc: %.6f, Testing Balanced Acc: %.6f')\\\n",
    "              % (epoch+1, EPOCHS, train_loss_[epoch], test_loss_[epoch], test_bal_loss_[epoch],\n",
    "                 train_acc_[epoch], test_acc_[epoch], test_bal_acc_[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get just the last epoch\n",
    "train_predictions = train_predictions_tmp[int(-len(train_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_train_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_predictions)):\n",
    "    d = torch.softmax(train_predictions[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_train_predictions.append(e[1])\n",
    "unpacked_train_predictions = np.array(unpacked_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7931251056351722\n",
      "(0.7116596638655462, 0.697195780807821, 0.7043534762833009)\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(train_y, unpacked_train_predictions)\n",
    "auc_train = metrics.auc(fpr, tpr)\n",
    "print(auc_train)\n",
    "PRF1 = metrics.precision_recall_fscore_support(train_y, unpacked_train_predictions.round(), average='binary')[:3]    \n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_predictions_tmp[int(-len(test_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_test_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_predictions)):\n",
    "    d = torch.softmax(test_predictions[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_test_predictions.append(e[1])\n",
    "unpacked_test_predictions = np.array(unpacked_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5286725058693016\n",
      "(0.16135084427767354, 0.5180722891566265, 0.24606580829756794)\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(test_y, unpacked_test_predictions)\n",
    "auc_test = metrics.auc(fpr, tpr)\n",
    "PRF1 = metrics.precision_recall_fscore_support(test_y, unpacked_test_predictions.round(), average='binary')[:3]    \n",
    "print(auc_test)\n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(unpacked_test_predictions.round() >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_bal = test_bal_predictions_tmp[int(-len(test_bal_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_test_predictions_bal = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_predictions_bal)):\n",
    "    d = torch.softmax(test_predictions_bal[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_test_predictions_bal.append(e[1])\n",
    "unpacked_test_predictions_bal = np.array(unpacked_test_predictions_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4803515625\n",
      "(0.5131578947368421, 0.4875, 0.5)\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(test_y_bal, unpacked_test_predictions_bal)\n",
    "auc_test_bal = metrics.auc(fpr, tpr)\n",
    "PRF1 = metrics.precision_recall_fscore_support(test_y_bal, unpacked_test_predictions_bal.round(), average='binary')[:3]    \n",
    "print(auc_test_bal)\n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=7770, minmax=(0.027971372, 0.98954076), mean=0.50069505, variance=0.05935071, skewness=0.08083004504442215, kurtosis=-0.9356530955840618)\n",
      "DescribeResult(nobs=1100, minmax=(0.03726772, 0.9887777), mean=0.5027398, variance=0.06375339, skewness=0.09460444003343582, kurtosis=-1.0724548020967584)\n",
      "DescribeResult(nobs=320, minmax=(0.035708405, 0.98607767), mean=0.50434315, variance=0.06758612, skewness=0.08775684237480164, kurtosis=-1.120683391391475)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.describe(unpacked_train_predictions))\n",
    "print(stats.describe(unpacked_test_predictions))\n",
    "print(stats.describe(unpacked_test_predictions_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_ = np.array([x.numpy() for x in train_acc_])\n",
    "test_acc_ = np.array([x.numpy() for x in test_acc_])\n",
    "test_bal_acc_ = np.array([x.numpy() for x in test_bal_acc_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ = np.array(train_loss_)\n",
    "test_loss_ = np.array(test_loss_)\n",
    "test_bal_loss_ = np.array(test_bal_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/train_acc', train_acc_)\n",
    "np.save('../data/test_acc_', test_acc_)\n",
    "np.save('../data/test_bal_acc_', test_bal_acc_)\n",
    "np.save('../data/train_loss_', train_loss_)\n",
    "np.save('../data/test_loss_', test_loss_)\n",
    "np.save('../data/test_bal_loss_', test_bal_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
