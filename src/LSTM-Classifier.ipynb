{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.001\n",
    "HIDDEN_DIM = 15\n",
    "GRADIENT_CLIP = 10\n",
    "DROPOUT_PARAM = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(TwoGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/SMSSpamCollection', sep='\\t', header=None)\n",
    "data['Text'] = data[1].str.replace('[^\\w\\s]','')\n",
    "data.columns = ['label', 'Full Text', 'Text']\n",
    "data['Lower Case Text'] = data['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(data['label'], return_counts=True)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(labels[np.argsort(-counts)])\n",
    "data['y'] = encoder.transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mask_train = np.random.random(data.shape[0]) < 0.8\n",
    "data_train = data[mask_train]\n",
    "data_test = data.iloc[~mask_train, :]\n",
    "\n",
    "\n",
    "#up sample data train for word2vec vocabulary, this must be done exactly was it was done for word2vec training\n",
    "countToIncrease = data_train[data_train['y'] == 0].shape[0] - data_train[data_train['y'] == 1].shape[0]\n",
    "spamupsampled = data_train[data_train['y'] == 1].sample(n=countToIncrease, replace=True)\n",
    "data_train_upsample_word2vec = pd.concat([spamupsampled, data_train]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "count_vect_sing_word = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "count_vect_sing_word.fit(data_train_upsample_word2vec['Lower Case Text'])\n",
    "tokenizer_word = count_vect_sing_word.build_tokenizer()\n",
    "\n",
    "#up sampling specificiallly to hve blanced data set and also match batch size\n",
    "countToIncrease_word = data_train[data_train['y'] == 0].shape[0]\n",
    "while (countToIncrease_word +  data_train[data_train['y'] == 0].shape[0]) % BATCH_SIZE != 0:\n",
    "    countToIncrease_word = countToIncrease_word + 1\n",
    "spamupsampled_word = data_train[data_train['y'] == 1].sample(n=countToIncrease_word, replace=True)\n",
    "data_train_upsample_word2vec = pd.concat([spamupsampled_word, data_train[data_train['y'] == 0]])\\\n",
    "                               .sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = len(data_test) % BATCH_SIZE\n",
    "data_test_spam = data_test[data_test['y'] == 1]\n",
    "\n",
    "data_test_downsample_ham = data_test[data_test['y'] == 0].sample(len(data_test[data_test['y'] == 0]) - diff)\n",
    "data_test_downsample = pd.concat([data_test_spam, data_test_downsample_ham]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data down sample balanced\n",
    "## won't work for general batch_sizes\n",
    "test_spam_count = data_test[data_test['y'] == 1].shape[0]\n",
    "diff = test_spam_count % BATCH_SIZE \n",
    "test_spam_count = test_spam_count - diff\n",
    "data_test_downsample_spam = data_test[data_test['y'] == 1].sample(test_spam_count)\n",
    "\n",
    "data_test_downsample_ham = data_test[data_test['y'] == 0].sample(test_spam_count)\n",
    "data_test_downsample_bal = pd.concat([data_test_downsample_ham, data_test_downsample_spam]) \\\n",
    "    .sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "CONTEXT_SIZE = 1\n",
    "VOCAB_SIZE = len(count_vect_sing_word.vocabulary_)\n",
    "word_to_ix = count_vect_sing_word.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = TwoGramLanguageModeler(VOCAB_SIZE, EMBEDDING_SIZE, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_WORD = '../data/word_2vec_model'\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "word2vec_model.load_state_dict(torch.load(MODEL_PATH_WORD))\n",
    "word2vec_model.eval()\n",
    "\n",
    "word_embeddings = word2vec_model.embeddings\n",
    "# TO FIX EMBEDDINGS\n",
    "word_embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect_sing_word is a CountVectorizer\n",
    "def _indicesForSentence(input_str, tokenizer = tokenizer_word, count_vect = count_vect_sing_word):\n",
    "    input_str = list(filter(lambda x: x in count_vect.vocabulary_, tokenizer(input_str)))\n",
    "    return torch.tensor([[word_to_ix[word]] for word in input_str], dtype=torch.long)\n",
    "\n",
    "def sentenceToNumpyInstance(input_str, embedder):\n",
    "    embeddings = embedder(_indicesForSentence(input_str))\n",
    "    if embeddings.shape == torch.Size([0]):\n",
    "        return np.zeros(EMBEDDING_SIZE)\n",
    "    else:\n",
    "        return torch.Tensor.numpy(embeddings.detach())\n",
    "    \n",
    "def word2vec_transform(data, embeddings, field = 'Lower Case Text'):\n",
    "    return np.array(data[field].apply(sentenceToNumpyInstance, embedder=embeddings).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data = word2vec_transform(data_train_upsample_word2vec, embeddings=word_embeddings)\n",
    "trans_test_data = word2vec_transform(data_test_downsample, embeddings=word_embeddings)\n",
    "trans_test_data_bal = word2vec_transform(data_test_downsample_bal, embeddings=word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentenceLengths(data):\n",
    "    sentence_lengths= []\n",
    "    for i in range(len(data)):\n",
    "        e = data[i]\n",
    "        if len(e.shape) > 1:\n",
    "            sentence_lengths.append(e.shape[0])\n",
    "        else:\n",
    "            sentence_lengths.append(1)\n",
    "    return sentence_lengths\n",
    "\n",
    "sentence_lengths = np.array(generateSentenceLengths(trans_data))\n",
    "indices_rv = np.argsort(-sentence_lengths)\n",
    "sentence_lengths = sentence_lengths[indices_rv]\n",
    "trans_data = trans_data[indices_rv]\n",
    "\n",
    "sentence_lengths_test = np.array(generateSentenceLengths(trans_test_data))\n",
    "indices_rv_test = np.argsort(-sentence_lengths_test)\n",
    "sentence_lengths_test = sentence_lengths_test[indices_rv_test]\n",
    "trans_test_data = trans_test_data[indices_rv_test]\n",
    "\n",
    "sentence_lengths_test_bal = np.array(generateSentenceLengths(trans_test_data_bal))\n",
    "indices_rv_test = np.argsort(-sentence_lengths_test_bal)\n",
    "sentence_lengths_test_bal = sentence_lengths_test_bal[indices_rv_test]\n",
    "trans_test_data_bal = trans_test_data_bal[indices_rv_test]\n",
    "\n",
    "\n",
    "assert sentence_lengths[0] >= sentence_lengths_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_so_far = -1\n",
    "for i in range(len(trans_data)):\n",
    "    e = trans_data[i]\n",
    "    if e.shape[0] >= max_len_so_far and len(e.shape) > 1:\n",
    "        max_len_so_far = e.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LEN = max_len_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = LEARNING_RATE * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim = EMBEDDING_SIZE, hidden_dim = HIDDEN_DIM, \\\n",
    "                 label_size = 2, batch_size = BATCH_SIZE, num_layers = 2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, dropout = DROPOUT_PARAM)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def forward(self, embeds, sentence_lengths):\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, sentence_lengths, batch_first=True)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        #lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        #y = self.hidden2label(lstm_out.float()[:, -1, :])\n",
    "        \n",
    "        # use h_t and last layer\n",
    "        y = self.hidden2label(self.hidden[0][-1].float())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSentences(data):\n",
    "    trans_data_reshape = np.zeros((data.shape[0], SENTENCE_LEN, EMBEDDING_SIZE))\n",
    "    # this will also do padding\n",
    "    for i in range(data.shape[0]):\n",
    "        e = data[i]\n",
    "        if len(e.shape) > 1:\n",
    "            sentence_len_sofar = e.shape[0]\n",
    "            for j in range(sentence_len_sofar):\n",
    "                trans_data_reshape[i, j] = e[j][0]\n",
    "    return trans_data_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data = padSentences(trans_data)\n",
    "trans_test_data = padSentences(trans_test_data)\n",
    "trans_test_data_bal = padSentences(trans_test_data_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data, sen_len):\n",
    "    return model(torch.tensor(data,dtype=torch.float), sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = data_train_upsample_word2vec['y']\n",
    "test_y = data_test_downsample['y']\n",
    "test_y_bal = data_test_downsample_bal['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "test_bal_loss_ = []\n",
    "train_acc_ = []\n",
    "test_acc_ = []\n",
    "test_bal_acc_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_LSTM = '../data/LSTM_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[Epoch:   1/ 50] Training Loss: 0.069371, Testing Loss: 0.068475, Test Balanced Loss: 0.069239, Training Acc: 0.501287, Testing Acc: 0.707273, Testing Balanced Acc: 0.490625\n",
      "1\n",
      "[Epoch:   2/ 50] Training Loss: 0.069256, Testing Loss: 0.068331, Test Balanced Loss: 0.069173, Training Acc: 0.509910, Testing Acc: 0.617273, Testing Balanced Acc: 0.518750\n",
      "2\n",
      "[Epoch:   3/ 50] Training Loss: 0.069034, Testing Loss: 0.069055, Test Balanced Loss: 0.069342, Training Acc: 0.534106, Testing Acc: 0.518182, Testing Balanced Acc: 0.493750\n",
      "3\n",
      "[Epoch:   4/ 50] Training Loss: 0.068428, Testing Loss: 0.069676, Test Balanced Loss: 0.069792, Training Acc: 0.559073, Testing Acc: 0.515455, Testing Balanced Acc: 0.493750\n",
      "4\n",
      "[Epoch:   5/ 50] Training Loss: 0.067519, Testing Loss: 0.071272, Test Balanced Loss: 0.070859, Training Acc: 0.575933, Testing Acc: 0.482727, Testing Balanced Acc: 0.468750\n",
      "5\n",
      "[Epoch:   6/ 50] Training Loss: 0.066334, Testing Loss: 0.072068, Test Balanced Loss: 0.071994, Training Acc: 0.600901, Testing Acc: 0.515455, Testing Balanced Acc: 0.521875\n",
      "6\n",
      "[Epoch:   7/ 50] Training Loss: 0.065653, Testing Loss: 0.074637, Test Balanced Loss: 0.072285, Training Acc: 0.606950, Testing Acc: 0.480909, Testing Balanced Acc: 0.496875\n",
      "7\n",
      "[Epoch:   8/ 50] Training Loss: 0.064725, Testing Loss: 0.079426, Test Balanced Loss: 0.074536, Training Acc: 0.618018, Testing Acc: 0.436364, Testing Balanced Acc: 0.503125\n",
      "8\n",
      "[Epoch:   9/ 50] Training Loss: 0.064037, Testing Loss: 0.079091, Test Balanced Loss: 0.075547, Training Acc: 0.629601, Testing Acc: 0.470909, Testing Balanced Acc: 0.512500\n",
      "9\n",
      "[Epoch:  10/ 50] Training Loss: 0.063738, Testing Loss: 0.080453, Test Balanced Loss: 0.079549, Training Acc: 0.633333, Testing Acc: 0.460909, Testing Balanced Acc: 0.478125\n",
      "10\n",
      "[Epoch:  11/ 50] Training Loss: 0.063145, Testing Loss: 0.081402, Test Balanced Loss: 0.077542, Training Acc: 0.639125, Testing Acc: 0.460909, Testing Balanced Acc: 0.512500\n",
      "11\n",
      "[Epoch:  12/ 50] Training Loss: 0.062236, Testing Loss: 0.084739, Test Balanced Loss: 0.081015, Training Acc: 0.648391, Testing Acc: 0.459091, Testing Balanced Acc: 0.465625\n",
      "12\n",
      "[Epoch:  13/ 50] Training Loss: 0.061467, Testing Loss: 0.086041, Test Balanced Loss: 0.086690, Training Acc: 0.657529, Testing Acc: 0.471818, Testing Balanced Acc: 0.471875\n",
      "13\n",
      "[Epoch:  14/ 50] Training Loss: 0.060972, Testing Loss: 0.087754, Test Balanced Loss: 0.083177, Training Acc: 0.662291, Testing Acc: 0.471818, Testing Balanced Acc: 0.493750\n",
      "14\n",
      "[Epoch:  15/ 50] Training Loss: 0.060471, Testing Loss: 0.090642, Test Balanced Loss: 0.083258, Training Acc: 0.670528, Testing Acc: 0.481818, Testing Balanced Acc: 0.506250\n",
      "15\n",
      "[Epoch:  16/ 50] Training Loss: 0.060341, Testing Loss: 0.089675, Test Balanced Loss: 0.081541, Training Acc: 0.667053, Testing Acc: 0.469091, Testing Balanced Acc: 0.531250\n",
      "16\n",
      "[Epoch:  17/ 50] Training Loss: 0.059096, Testing Loss: 0.091337, Test Balanced Loss: 0.088759, Training Acc: 0.674646, Testing Acc: 0.473636, Testing Balanced Acc: 0.490625\n",
      "17\n",
      "[Epoch:  18/ 50] Training Loss: 0.058009, Testing Loss: 0.096137, Test Balanced Loss: 0.091673, Training Acc: 0.683398, Testing Acc: 0.489091, Testing Balanced Acc: 0.506250\n",
      "18\n",
      "[Epoch:  19/ 50] Training Loss: 0.057345, Testing Loss: 0.096188, Test Balanced Loss: 0.085614, Training Acc: 0.690862, Testing Acc: 0.498182, Testing Balanced Acc: 0.543750\n",
      "19\n",
      "[Epoch:  20/ 50] Training Loss: 0.056552, Testing Loss: 0.101609, Test Balanced Loss: 0.089577, Training Acc: 0.690605, Testing Acc: 0.455455, Testing Balanced Acc: 0.525000\n",
      "20\n",
      "[Epoch:  21/ 50] Training Loss: 0.055889, Testing Loss: 0.098039, Test Balanced Loss: 0.093339, Training Acc: 0.697426, Testing Acc: 0.499091, Testing Balanced Acc: 0.528125\n",
      "21\n",
      "[Epoch:  22/ 50] Training Loss: 0.055272, Testing Loss: 0.101316, Test Balanced Loss: 0.090925, Training Acc: 0.695238, Testing Acc: 0.473636, Testing Balanced Acc: 0.503125\n",
      "22\n",
      "[Epoch:  23/ 50] Training Loss: 0.054058, Testing Loss: 0.108320, Test Balanced Loss: 0.097440, Training Acc: 0.701287, Testing Acc: 0.483636, Testing Balanced Acc: 0.518750\n",
      "23\n",
      "[Epoch:  24/ 50] Training Loss: 0.053679, Testing Loss: 0.106997, Test Balanced Loss: 0.093086, Training Acc: 0.695109, Testing Acc: 0.472727, Testing Balanced Acc: 0.503125\n",
      "24\n",
      "[Epoch:  25/ 50] Training Loss: 0.053016, Testing Loss: 0.113602, Test Balanced Loss: 0.101645, Training Acc: 0.700515, Testing Acc: 0.495455, Testing Balanced Acc: 0.518750\n",
      "25\n",
      "[Epoch:  26/ 50] Training Loss: 0.052537, Testing Loss: 0.101635, Test Balanced Loss: 0.085066, Training Acc: 0.702960, Testing Acc: 0.511818, Testing Balanced Acc: 0.515625\n",
      "26\n",
      "[Epoch:  27/ 50] Training Loss: 0.052354, Testing Loss: 0.112197, Test Balanced Loss: 0.093357, Training Acc: 0.706435, Testing Acc: 0.488182, Testing Balanced Acc: 0.528125\n",
      "27\n",
      "[Epoch:  28/ 50] Training Loss: 0.050721, Testing Loss: 0.124373, Test Balanced Loss: 0.103068, Training Acc: 0.718919, Testing Acc: 0.489091, Testing Balanced Acc: 0.500000\n",
      "28\n",
      "[Epoch:  29/ 50] Training Loss: 0.050678, Testing Loss: 0.118938, Test Balanced Loss: 0.100785, Training Acc: 0.714286, Testing Acc: 0.466364, Testing Balanced Acc: 0.543750\n",
      "29\n",
      "[Epoch:  30/ 50] Training Loss: 0.050687, Testing Loss: 0.118657, Test Balanced Loss: 0.099285, Training Acc: 0.724968, Testing Acc: 0.493636, Testing Balanced Acc: 0.509375\n",
      "30\n",
      "[Epoch:  31/ 50] Training Loss: 0.050414, Testing Loss: 0.116068, Test Balanced Loss: 0.105217, Training Acc: 0.718919, Testing Acc: 0.512727, Testing Balanced Acc: 0.484375\n",
      "31\n",
      "[Epoch:  32/ 50] Training Loss: 0.050128, Testing Loss: 0.117793, Test Balanced Loss: 0.100440, Training Acc: 0.712870, Testing Acc: 0.487273, Testing Balanced Acc: 0.496875\n",
      "32\n",
      "[Epoch:  33/ 50] Training Loss: 0.049051, Testing Loss: 0.125246, Test Balanced Loss: 0.108790, Training Acc: 0.724968, Testing Acc: 0.471818, Testing Balanced Acc: 0.496875\n",
      "33\n",
      "[Epoch:  34/ 50] Training Loss: 0.049175, Testing Loss: 0.116426, Test Balanced Loss: 0.107760, Training Acc: 0.720849, Testing Acc: 0.508182, Testing Balanced Acc: 0.540625\n",
      "34\n",
      "[Epoch:  35/ 50] Training Loss: 0.049242, Testing Loss: 0.116963, Test Balanced Loss: 0.109530, Training Acc: 0.725483, Testing Acc: 0.502727, Testing Balanced Acc: 0.503125\n",
      "35\n",
      "[Epoch:  36/ 50] Training Loss: 0.048889, Testing Loss: 0.122423, Test Balanced Loss: 0.114716, Training Acc: 0.721622, Testing Acc: 0.498182, Testing Balanced Acc: 0.468750\n",
      "36\n",
      "[Epoch:  37/ 50] Training Loss: 0.048873, Testing Loss: 0.142211, Test Balanced Loss: 0.113725, Training Acc: 0.720077, Testing Acc: 0.484545, Testing Balanced Acc: 0.496875\n",
      "37\n",
      "[Epoch:  38/ 50] Training Loss: 0.048511, Testing Loss: 0.132507, Test Balanced Loss: 0.111793, Training Acc: 0.720077, Testing Acc: 0.459091, Testing Balanced Acc: 0.503125\n",
      "38\n",
      "[Epoch:  39/ 50] Training Loss: 0.048214, Testing Loss: 0.126497, Test Balanced Loss: 0.104133, Training Acc: 0.723938, Testing Acc: 0.494545, Testing Balanced Acc: 0.500000\n",
      "39\n",
      "[Epoch:  40/ 50] Training Loss: 0.048202, Testing Loss: 0.124948, Test Balanced Loss: 0.111742, Training Acc: 0.725997, Testing Acc: 0.500000, Testing Balanced Acc: 0.465625\n",
      "40\n",
      "[Epoch:  41/ 50] Training Loss: 0.048171, Testing Loss: 0.123003, Test Balanced Loss: 0.102689, Training Acc: 0.719691, Testing Acc: 0.501818, Testing Balanced Acc: 0.506250\n",
      "41\n",
      "[Epoch:  42/ 50] Training Loss: 0.047595, Testing Loss: 0.129605, Test Balanced Loss: 0.105860, Training Acc: 0.733848, Testing Acc: 0.493636, Testing Balanced Acc: 0.550000\n",
      "42\n",
      "[Epoch:  43/ 50] Training Loss: 0.048261, Testing Loss: 0.119902, Test Balanced Loss: 0.104279, Training Acc: 0.726898, Testing Acc: 0.490000, Testing Balanced Acc: 0.515625\n",
      "43\n",
      "[Epoch:  44/ 50] Training Loss: 0.047622, Testing Loss: 0.138306, Test Balanced Loss: 0.114584, Training Acc: 0.726641, Testing Acc: 0.474545, Testing Balanced Acc: 0.525000\n",
      "44\n",
      "[Epoch:  45/ 50] Training Loss: 0.047000, Testing Loss: 0.141755, Test Balanced Loss: 0.124725, Training Acc: 0.733333, Testing Acc: 0.499091, Testing Balanced Acc: 0.490625\n",
      "45\n",
      "[Epoch:  46/ 50] Training Loss: 0.048024, Testing Loss: 0.127806, Test Balanced Loss: 0.102788, Training Acc: 0.729987, Testing Acc: 0.490000, Testing Balanced Acc: 0.531250\n",
      "46\n",
      "[Epoch:  47/ 50] Training Loss: 0.046731, Testing Loss: 0.140281, Test Balanced Loss: 0.115447, Training Acc: 0.734492, Testing Acc: 0.483636, Testing Balanced Acc: 0.496875\n",
      "47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  48/ 50] Training Loss: 0.046719, Testing Loss: 0.138874, Test Balanced Loss: 0.123964, Training Acc: 0.730759, Testing Acc: 0.491818, Testing Balanced Acc: 0.440625\n",
      "48\n",
      "[Epoch:  49/ 50] Training Loss: 0.047021, Testing Loss: 0.132580, Test Balanced Loss: 0.115219, Training Acc: 0.727671, Testing Acc: 0.497273, Testing Balanced Acc: 0.481250\n",
      "49\n",
      "[Epoch:  50/ 50] Training Loss: 0.046897, Testing Loss: 0.119919, Test Balanced Loss: 0.107219, Training Acc: 0.731403, Testing Acc: 0.513636, Testing Balanced Acc: 0.478125\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "train_predictions_tmp = []\n",
    "test_predictions_tmp = []\n",
    "test_bal_predictions_tmp = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print(epoch)\n",
    "    # using adam so might be too much\n",
    "    #optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_data.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_data[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(train_y.iloc[this_batch].values))\n",
    "        #inner_y.weight.requires_grad = False\n",
    "        batch_lengths = sentence_lengths[this_batch]\n",
    "        model.zero_grad()\n",
    "        \n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        train_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    train_loss_.append(1.0 * total_loss / total)\n",
    "    train_acc_.append(1.0 * total_acc.float() / total)\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_test_data.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_test_data[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(test_y.iloc[this_batch].values))\n",
    "        batch_lengths = sentence_lengths_test[this_batch]\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        test_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    test_loss_.append(total_loss / total)\n",
    "    test_acc_.append(total_acc.float() / total)\n",
    "    \n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    batch_indices = chunks(range(trans_test_data_bal.shape[0]), BATCH_SIZE)\n",
    "    for this_batch in batch_indices:\n",
    "        this_batch = list(this_batch)\n",
    "        inner_data = Variable(torch.tensor(trans_test_data_bal[this_batch], requires_grad=False))\n",
    "        inner_y = Variable(torch.tensor(test_y_bal.iloc[this_batch].values))\n",
    "        batch_lengths = sentence_lengths_test_bal[this_batch]\n",
    "\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = prediction(inner_data, batch_lengths)\n",
    "        loss = loss_function(output, inner_y)\n",
    "        test_bal_predictions_tmp.append(output.data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == inner_y).sum()\n",
    "        total += len(inner_y)\n",
    "        total_loss += loss.data.item()\n",
    "    test_bal_loss_.append(total_loss / total)\n",
    "    test_bal_acc_.append(total_acc.float() / total)\n",
    "\n",
    "    print(('[Epoch: %3d/%3d] Training Loss: %.6f, Testing Loss: %.6f, Test Balanced Loss: %.6f, '+\n",
    "          'Training Acc: %.6f, Testing Acc: %.6f, Testing Balanced Acc: %.6f')\\\n",
    "              % (epoch+1, EPOCHS, train_loss_[epoch], test_loss_[epoch], test_bal_loss_[epoch],\n",
    "                 train_acc_[epoch], test_acc_[epoch], test_bal_acc_[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get just the last epoch\n",
    "train_predictions = train_predictions_tmp[int(-len(train_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_train_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_predictions)):\n",
    "    d = torch.softmax(train_predictions[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_train_predictions.append(e[1])\n",
    "unpacked_train_predictions = np.array(unpacked_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8370847746812957\n",
      "(0.7353556485355649, 0.7234370980190378, 0.7293476851251459)\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(train_y, unpacked_train_predictions)\n",
    "auc_train = metrics.auc(fpr, tpr)\n",
    "print(auc_train)\n",
    "PRF1 = metrics.precision_recall_fscore_support(train_y, unpacked_train_predictions.round(), average='binary')[:3]    \n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_predictions_tmp[int(-len(test_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_test_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_predictions)):\n",
    "    d = torch.softmax(test_predictions[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_test_predictions.append(e[1])\n",
    "unpacked_test_predictions = np.array(unpacked_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5168145816671397\n",
      "(0.15514018691588785, 0.5, 0.23680456490727533)\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(test_y, unpacked_test_predictions)\n",
    "auc_test = metrics.auc(fpr, tpr)\n",
    "PRF1 = metrics.precision_recall_fscore_support(test_y, unpacked_test_predictions.round(), average='binary')[:3]    \n",
    "print(auc_test)\n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(unpacked_test_predictions.round() >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_bal = test_bal_predictions_tmp[int(-len(test_bal_predictions_tmp)/EPOCHS):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_test_predictions_bal = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_predictions_bal)):\n",
    "    d = torch.softmax(test_predictions_bal[i], dim = 1).numpy()\n",
    "    for e in d:\n",
    "        unpacked_test_predictions_bal.append(e[1])\n",
    "unpacked_test_predictions_bal = np.array(unpacked_test_predictions_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49890625\n",
      "(0.47878787878787876, 0.49375, 0.48615384615384616)\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(test_y_bal, unpacked_test_predictions_bal)\n",
    "auc_test_bal = metrics.auc(fpr, tpr)\n",
    "PRF1 = metrics.precision_recall_fscore_support(test_y_bal, unpacked_test_predictions_bal.round(), average='binary')[:3]    \n",
    "print(auc_test_bal)\n",
    "print(PRF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=7770, minmax=(0.00040118548, 0.9996061), mean=0.5008003, variance=0.08769157, skewness=0.009073569439351559, kurtosis=-0.8040775144829739)\n",
      "DescribeResult(nobs=1100, minmax=(0.00051295856, 0.99946624), mean=0.4968473, variance=0.10723896, skewness=0.008837752044200897, kurtosis=-1.2795007820589432)\n",
      "DescribeResult(nobs=320, minmax=(0.0010298186, 0.99967134), mean=0.51529443, variance=0.080791734, skewness=0.015830067917704582, kurtosis=-0.8527219587166686)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.describe(unpacked_train_predictions))\n",
    "print(stats.describe(unpacked_test_predictions))\n",
    "print(stats.describe(unpacked_test_predictions_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_ = np.array([x.numpy() for x in train_acc_])\n",
    "test_acc_ = np.array([x.numpy() for x in test_acc_])\n",
    "test_bal_acc_ = np.array([x.numpy() for x in test_bal_acc_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ = np.array(train_loss_)\n",
    "test_loss_ = np.array(test_loss_)\n",
    "test_bal_loss_ = np.array(test_bal_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/train_acc', train_acc_)\n",
    "np.save('../data/test_acc_', test_acc_)\n",
    "np.save('../data/test_bal_acc_', test_bal_acc_)\n",
    "np.save('../data/train_loss_', train_loss_)\n",
    "np.save('../data/test_loss_', test_loss_)\n",
    "np.save('../data/test_bal_loss_', test_bal_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
